---
engine: knitr
---

# Linear models {#sec-its-just-a-linear-model}


<! -- PL: OK so these comments are based purely on first impression of this chapter alone, so take with a pinch of salt. I will go back and read the introduction once I have more time! --> 

<! -- one suggestion. would be good to get a read of this chapter from some students or someone who wants to learn this but doesn't know, for the sake of testing comprehenibility. not able to imagine what it's like to not know anymore. Again, once i have more time i'll maybe try this with the post stratification chapter since aside from some toy analysis with your code like 3 years ago, i haven't thought about it since... -->



**Required material**

- Read *Regression and Other Stories*, Chapters 6 "Background on regression modeling", 7 "Linear regression with a single predictor", 10 "Linear regression with multiple predictors", 13 "Logistic regression", and 15 "Other generalized linear models", [@gelmanhillvehtari2020]
- Read *An Introduction to Statistical Learning with Applications in R*, Chapters 3 "Linear Regression", and 4 "Classification", [@islr]
- Read *We Gave Four Good Pollsters the Same Raw Data. They Had Four Different Results*, [@cohn2016]
- Read *Why most published research findings are false*, [@ioannidis2005most]
- Read *Machine learning is going real-time*, [@chiphuyenone]
- Watch *Democratizing R with Plumber APIs*, [@democratizingr]
- Read *Science Storms the Cloud*, [@Gentemann2021]

**Key concepts and skills**

- Linear models are a key component of statistical inference and enable us to consider a wide range of circumstance. Simple and multiple linear regression refer to the situation in which we consider some continuous dependent variable as a function of one, and multiple, independent variables, respectively. 
- Logistic regression is used when the dependent variable is a binary variable, and a Poisson regression is used for a count variable. <! -- repetition of respectively jarring. moving glm to its own bullet. -->
- We are concerned with two different aspects: prediction and inference. We use machine learning approaches for the former, and Bayesian methods for the latter. 
- Finally, putting a model into production requires a different set of skills to building it. This includes a familiarity with a cloud provider and the ability to create an API. <! -- i'd just be a little more specific if possible about what production means in this instance, otherwise you might get the impression that typing in lm(y~z) requires 'skill'.  -->



**Key packages and functions**

- Base R [@citeR]
  - `binomial()`
  - `glm()`
  - `lm()`
  - `rexp()`
  - `rnorm()`
  - `rpois()`
  - `sample()`
  - `set.seed()`
  - `summary()`
- `analogsea` [@citeanalogsea]
  - `debian_apt_get_install()`
  - `droplets()`
  - `install_github()`
  - `install_r_package()`
  - `key_create()`
- `beepr` [@beepr]
  - `beep()`
- `broom` [@broom]
  - `augment()`
  - `glance()`
  - `tidy()`
- `gutenbergr` [@gutenbergr]
	- `gutenberg_download()`
	- `gutenberg_works()`
- `marginaleffects` [@marginaleffects]
  - `marginaleffects()`
- `modelsummary` [@citemodelsummary]
  - `modelsummary()`
- `plumber` [@plumber]
  - `do_deploy_api()`
  - `do_provision()`
  - `plumb()`
- `plumberDeploy` [@plumberdeploy]
- `poissonreg` [@poissonreg]
  - `poisson_reg()`
- `rstanarm` [@citerstanarm]
	- `default_prior_coef()`
	- `default_prior_intercept()`
	- `exponential()`
	- `gaussian()`
	- `loo()`
  - `loo_compare()`
	- `posterior_vs_prior()`
	- `pp_check()`
	- `prior_summary()`
	- `stan_glm()`
- `ssh` [@ssh]
  - `ssh_key_info()`
- `tidymodels` [@citeTidymodels]
  - `parsnip` [@parsnip]
    - `fit()`
    - `linear_reg()`
    - `logistic_reg()`
    - `poisson_reg()`
    - `set_engine()`
  - `recipes` [@recipes]
    - `recipe()`
  - `rsample` [@rsample]
    - `initial_split()`
    - `testing()`
    - `training()`
    - `vfold_cv()`
  - `tune` [@tune]    
    - `collect_metrics()`
    - `collect_predictions()`
    - `conf_mat_resampled()`
    - `fit_resamples()`
  - `yarkdstick` [@yardstick]    
    - `conf_mat()`



## Introduction

Linear models have been used in various forms for a long time. For instance, @stigler [p.16] describes how least squares, which is a method to fit simple linear regression, was associated with foundational problems in astronomy in the 1700s, such as determining the motion of the moon and reconciling the non-periodic motion of Jupiter and Saturn. The fundamental issue at the time with least squares was that of hesitancy to combine different observations. Astronomers were early to develop a comfort with doing this because they had typically gathered their observations themselves and knew that the conditions of the data gathering were similar, even though the value of the observation was different. It took longer for social scientists to become comfortable with linear models, possibly because they were hesitant to group together data they worried were not alike. In this one sense, astronomers had an advantage because they could compare their predictions with what happened whereas this was more difficult for social scientists [@stigler, p. 163]. 

Linear models have evolved substantially over the past century. Francis Galton, mentioned in @sec-hunt-data, and others of his generation, some of whom were eugenicists, used linear regression in earnest in the late 1800s and early 1900s. Binary outcomes quickly became of interest and needed special treatment, leading to the development and wide adaption of logistic regression and similar methods in the mid-1900s [@cramer2002origins]. The generalized linear model framework came into being, in a formal sense, in the 1970s with @nelder1972generalized. Generalized linear models (GLMs) broaden the types of outcomes that are allowed. We still model outcomes as a linear function, but we are no longer constrained by the normal distribution. The outcome can be anything in the exponential family, and popular choices include the logistic distribution and the Poisson distribution. A further generalization of GLMs is generalized additive models (GAMs) where we broaden the structure of the explanatory side. We still explain the dependent variable as an additive function of various bits and pieces, but those bits and pieces can be functions. This framework was proposed in the 1990s by @hastie1990generalized.

:::{.callout-note}
## Shoulders of giants

Dr Robert Tibshirani is Professor in the Departments of Statistics and Biomedical Data Science at Stanford University. After taking a PhD in Statistics from Stanford University in 1981, he joined the University of Toronto as an assistant professor. He was promoted to full professor in 1994 and moved to Stanford in 1998. He made fundamental contributions including GAMs, mentioned above, and lasso regression, which is a way of automated variable selection and will be covered in @sec-text-as-data. He is an author of @islr. He was awarded the COPSS Presidents' Award in 1996 and was appointed a Fellow of the Royal Society in 2019.
:::

When we build models, we are not discovering "the truth". A model is not, and cannot be, a true representation of reality. We are using the model to help us explore and understand our data. There is no one best model, there are just useful models that help us learn something about the data that we have and hence, hopefully, something about the world from which the data were generated. When we use models, we are trying to understand the world, but there are enormous constraints on the perspective we bring to this. Further, we should not just blindly throw data into a regression model and hope that it will sort it out---"Regression will not sort it out. Regression is indeed an oracle, but a cruel one. It speaks in riddles and delights in punishing us for asking bad questions" [@citemcelreath, p. 162].

We use models to understand the world. We poke, push, and test them. We build them and rejoice in their beauty, and then seek to understand their limits and ultimately destroy them. It is this process that is important, it is this process that allows us to better understand the world; not the outcome. When we build models, we need to keep in mind both the world of the model and the broader world that we want to be able to speak about. To what extent does a model trained on the experiences of straight, cis, men, speak to the world as it is? It is not worthless, but it is also not unimpeachable. To what extent does the model teach us about the data that we have? To what extent do the data that we have reflect the world about which we would like to draw conclusions? We need to keep such questions front of mind.

A lot of statistical methods still commonly used today were developed for situations such as astronomy and agriculture. People such as @fisherarrangement were able to randomize the order of fields and planting because they worked at agricultural stations. But many of the subsequent applications in the twentieth and twenty-first centuries do not have those properties. Statistics is often taught as though it proceeds through some idealized process where a hypothesis appears, is tested against some data that similarly appears, and is either confirmed or not. But that is not what happens. We react to incentives. We dabble, guess, and test, and then follow our intuition, backfilling as we need. All of this is fine. But it is not a world in which a traditional null hypothesis holds completely. This means concepts such as p-values and power lose some of their meaning. While we need to understand these foundations, we also need to be sophisticated enough to know when we need to move away from them.

Manual statistical checks are widely used in modelling. And an extensive suite of them is available. But automated testing is also important. For instance, @wakefieldestimates built a model of excess deaths in a variety of countries, to estimate the overall death toll from the pandemic. After initially releasing the model, which had been extensively manually checked for statistical issues and reasonableness, some of the results were re-examined and it was found that the estimates for Germany and Sweden were over-sensitive. The authors acknowledged this, and addressed the issues, but the integration of automated testing of expected values for the coefficients, in addition to the usual manual statistical checks, would go some way to enabling us to have more faith in the models of others.

In this chapter we begin with simple linear regression, and then move to multiple linear regression, the difference being the number of explanatory variables that we allow. We then consider logistic, Poisson, and negative binomial regression. We go through three approaches for each of these: base R, in particular the `lm()` and `glm()` functions, which are useful when we want to quickly use the models in EDA; `tidymodels` [@citeTidymodels] which is useful when we are interested in prediction; and `rstanarm` [@citerstanarm] for when we are interested in inference. In general, a model is either optimized for prediction or inference. Regardless of the approach we use, the important thing to remember is that modelling in this way is just fancy averaging and reflects the dataset.


## Simple linear regression

When we are interested in the relationship of some continuous variable, say $y$, and some other variable, say $x$, we can use simple linear regression. This is based on the Normal, also called "Gaussian", distribution, but it is not these variables themselves that are normally distributed. The shape of the Normal distribution is determined by two parameters, the mean, $\mu$, and the standard deviation, $\sigma$ [@pitman, p. 94]: 

$$y = \frac{1}{\sqrt{2\pi\sigma}}e^{-\frac{1}{2}z^2},$$
where $z = (x - \mu)/\sigma$ is the difference between the mean and $x$, scaled by the standard deviation.

As introduced in @sec-r-essentials, we use `rnorm()` to simulate data from the Normal distribution. 

```{r}
#| message: false
#| warning: false

library(tidyverse)

set.seed(853)

normal_example <-
  tibble(draws = rnorm(n = 20, mean = 0, sd = 1))

normal_example |> pull(draws)
```

Here we specified 20 draws from a Normal distribution with mean of 0 and standard deviation of 1. When we deal with real data, we will not know these parameters and we want to use our data to estimate them. We can estimate the mean, $\bar{x}$, and standard deviation, $\hat{\sigma}_x$, with the following estimators:

$$
\begin{aligned}
 \bar{x} &= \frac{1}{n} \times \sum_{i = 1}^{n}x_i\\
 \hat{\sigma}_{x} &= \sqrt{\frac{1}{n} \times \sum_{i = 1}^{n}\left(x_i - \bar{x}\right)^2}
\end{aligned}
$$

If $\hat{\sigma}_x$ is the estimate of the standard deviation, then the standard error of $\bar{x}$ is:
$$\mbox{SE}(\bar{x}) = \frac{\hat{\sigma}_{x}}{\sqrt{n}}.$$

We can implement these in code using our simulated data to see how close our estimates are.

```{r}
#| echo: true
#| eval: true
#| label: tbl-meanstdexample
#| tbl-cap: "Estimates of the mean and standard deviation based on the simulated data"

estimated_mean <-
  sum(normal_example$draws) / nrow(normal_example)

normal_example <-
  normal_example |>
  mutate(diff_square = (draws - estimated_mean)^2)

estimated_standard_deviation <-
  sqrt(sum(normal_example$diff_square) / nrow(normal_example))

knitr::kable(
  tibble(
    mean = estimated_mean,
    sd = estimated_standard_deviation
  ),
  col.names = c(
    "Estimated mean",
    "Estimated standard deviation"
  ),
  digits = 2,
  align = c("l", "r")
)
```

We should not be too worried that our estimates are slightly off (@tbl-meanstdexample). We only considered 20 observations. It will typically take a larger number of draws before we get the expected shape, and our estimated parameters get close to the actual parameters, but it will almost surely happen (@fig-normaldistributiontakingshape). @wasserman [p. 76] considers our certainty of this, which is due to the Law of Large Numbers, as a nonpareil accomplishment of probability.

```{r}
#| eval: true
#| fig-cap: "The Normal distribution takes its familiar shape as the number of draws increases"
#| include: true
#| label: fig-normaldistributiontakingshape
#| message: false
#| warning: false

set.seed(853)

normal_takes_shape <-
  tibble(
    number_draws = c(),
    draws = c()
  )

for (i in c(2, 5, 10, 50, 100, 500, 1000, 10000, 100000)) {
  draws_i <-
    tibble(
      number_draws = c(rep.int(
        x = paste(as.integer(i), " draws"),
        times = i
      )),
      draws = c(rnorm(
        n = i,
        mean = 0,
        sd = 1
      ))
    )

  normal_takes_shape <- rbind(normal_takes_shape, draws_i)
  rm(draws_i)
}

normal_takes_shape |>
  mutate(number_draws = as_factor(number_draws)) |>
  ggplot(aes(x = draws)) +
  geom_density() +
  theme_minimal() +
  facet_wrap(
    vars(number_draws),
    scales = "free_y"
  ) +
  labs(
    x = "Draw",
    y = "Density"
  )
```

When we use simple linear regression, we assume that our relationship is characterized by the variables and the parameters. If we have two variables, $Y$ and $X$, then we could characterize a linear relationship between these as:
$$
Y = \beta_0 + \beta_1 X + \epsilon.
$$ {#eq-xandy}

<! -- appreciate that if you're reading this as an 'end user', you probably already have a fairly good understanding of linear regression. But, given that most people's first understanding of the linear model is the linear equation, it might make sense to start with that, then move on to the stochastic elements and distributional assumptions, so normal distribution, logit transformation etc.   -->

Here, there are two coefficients, also referred to as parameters: the intercept, $\beta_0$, and the slope, $\beta_1$. In @eq-xandy we are saying that the expected value of $Y$ is $\beta_0$ when $X$ is 0, and that the expected value of $Y$ will change by $\beta_1$ units for every one unit change in $X$. We may then take this relationship to the data that we have to estimate these coefficients. The $\epsilon$ is noise and accounts for deviations away from this relationship. It is this noise that we generally assume to be normally distributed.

<! -- maybe a lil subhed here -->

To make this example concrete, we will simulate some data and then discuss it in that context. We could consider the time it takes someone to run five kilometers, compared with the time it takes them to run a marathon (@fig-fivekmvsmarathon-1). In the simulation we specify a relationship of 8.4, as that is roughly the ratio between the distance of a marathon and a five-kilometer run. Notice that it is the noise that is normally distributed, not the variables.

```{r}
#| eval: true
#| include: true
#| label: fig-simulatemarathondata
#| message: false
#| warning: false

set.seed(853)

number_of_observations <- 200
expected_relationship <- 8.4
very_fast_5km_time <- 15
good_enough_5km_time <- 30

sim_run_data <-
  tibble(
    five_km_time =
      runif(
        n = number_of_observations,
        min = very_fast_5km_time,
        max = good_enough_5km_time
      ),
    noise = rnorm(n = number_of_observations, mean = 0, sd = 20),
    marathon_time = five_km_time * expected_relationship + noise
  ) |>
  mutate(
    five_km_time = round(x = five_km_time, digits = 1),
    marathon_time = round(x = marathon_time, digits = 1)
  ) |>
  select(-noise)

sim_run_data
```


```{r}
#| echo: true
#| eval: true
#| message: false
#| warning: false
#| label: fig-fivekmvsmarathon
#| fig-cap: "Simulated data of the relationship between the time to run five kilometers and a marathon"
#| fig-subcap: ["Distribution of simulated data", "With one 'linear best-fit' line illustrating the implied relationship", "Including standard errors"]
#| layout-ncol: 2

base_sim_run_data <-
  sim_run_data |>
  ggplot(aes(x = five_km_time, y = marathon_time)) +
  geom_point(alpha = 0.5) +
  labs(
    x = "Five-kilometer time (minutes)",
    y = "Marathon time (minutes)"
  ) +
  theme_classic()

base_and_fit_sim_run_data <-
  base_sim_run_data +
  geom_smooth(
    method = "lm",
    se = FALSE,
    color = "black",
    linetype = "dashed",
    formula = "y ~ x"
  )

base_and_fit_and_se_sim_run_data <-
  base_sim_run_data +
  geom_smooth(
    method = "lm",
    se = TRUE,
    color = "black",
    linetype = "dashed",
    formula = "y ~ x"
  )

base_sim_run_data

base_and_fit_sim_run_data

base_and_fit_and_se_sim_run_data
```

In this simulated example, we know the true values of $\beta_0$ and $\beta_1$, which are 0 and 8.4, respectively. But our challenge is to see if we can use only the data, and simple linear regression, to recover them. That is, can we use $x$, which is the five-kilometer time, to produce estimates of $y$, which is the marathon time, and that we denote by $\hat{y}$ (by convention, hats are used to indicate that these are, or will be, estimated values) [@islr, p. 61]:

$$\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x.$$

This involves estimating values for $\beta_0$ and $\beta_1$. But how should we estimate these coefficients? Even if we impose a linear relationship there are many options because many straight lines could be drawn. But some of those lines would fit the data better than others.

One way we may define a line as being "better" than another, is if it is as close as possible to each of the $x$ and $y$ combinations that we know. There are a lot of candidates for how we define as close as possible, but one is to minimize the residual sum of squares. To do this we produce estimates for $\hat{y}$ based on some guesses of $\hat{\beta}_0$ and $\hat{\beta}_1$, given the $x$. We then work out how wrong, for every observation $i$, we were [@islr, p. 62]:
$$e_i = y_i - \hat{y}_i.$$

To compute the residual sum of squares (RSS), we sum the errors across all the points, taking the square to account for negative differences [@islr, p. 62]:
$$\mbox{RSS} = e^2_1+ e^2_2 +\dots + e^2_n.$$
This results in one linear best-fit line (@fig-fivekmvsmarathon-2), but it is worth reflecting on all the assumptions and decisions that it took to get us to this point.

Underpinning our use of simple linear regression is a belief that there is some "true" relationship between $X$ and $Y$. And that this is a linear function of $X$. We do not, and cannot, know the "true" relationship between $X$ and $Y$. All we can do is use our sample to estimate it. But because our understanding depends on that sample, for every possible sample, we would get a slightly different relationship, as measured by the coefficients. 

That $\epsilon$ is a measure of our error---what does the model not know in the small, contained, world implied by this dataset? There is going to be plenty that the model does not know, but we hope the error does not depend on $X$, and that the error is normally distributed.

We can conduct simple linear regression with `lm()` from base R. We specify the dependent variable first, then `~`, followed by the independent variable. The dependent variable is the variable of interest, while the dependent variable is the basis on which we consider that variable. Finally, we specify the dataset. Before we run a regression, we may want to include a quick check of the class of the variables, and the number of observations, just to ensure that it corresponds with what we were expecting. And after we run it, we may check that our estimate seems reasonable. For instance, (pretending that we had not ourselves imposed this in the simulation) based on our knowledge of the respective distances of a five-kilometer run and a marathon, we would expect $\beta_1$ to be somewhere between 6 and 10.

```{r}
# Check the class and number of observations are as expected
stopifnot(
  class(sim_run_data$marathon_time) == "numeric",
  class(sim_run_data$five_km_time) == "numeric",
  nrow(sim_run_data) == 200
)

sim_run_data_first_model <-
  lm(
    marathon_time ~ five_km_time,
    data = sim_run_data
  )

stopifnot(between(
  sim_run_data_first_model$coefficients[2],
  6,
  10
))
```

To see the result of the regression, we can use `modelsummary()` from `modelsummary` [@citemodelsummary] (@tbl-modelsummaryfivekmonly). We focus on those in the first column.

```{r}
sim_run_data <-
  sim_run_data |>
  mutate(centered_five_km_time = five_km_time - mean(sim_run_data$five_km_time))

sim_run_data_centered_model <-
  lm(marathon_time ~ centered_five_km_time,
   data = sim_run_data)
```



```{r}
#| label: tbl-modelsummaryfivekmonly
#| tbl-cap: "Explaining marathon times based on five-kilometer run times"

modelsummary::modelsummary(
  list(
    "Five km only" = sim_run_data_first_model,
    "Five km only, centered" = sim_run_data_centered_model
  ),
  fmt = 2,
  statistic = "conf.int"
)

```

The top half of the table provides our estimated coefficients and standard errors in brackets. And the second half provides some useful diagnostics, which we will not dwell too much on here. The intercept in the left column is the marathon time associated with a hypothetical five-kilometer time of 0 minutes. Hopefully this example illustrates the need to always interpret the intercept coefficient carefully. And to disregard it at times. For instance, in this circumstance, we know that the intercept should be zero, and it is just being set to around four because that is the best-fit given all the observations were for five-kilometer times between 15 and 30. 

The intercept becomes more interpretable when we run the regression using centered five-kilometer times. That is, for each of the five-kilometer times we minus the mean five-kilometer time. In this case, the intercept is interpreted as the expected marathon time for someone who runs five kilometers in the average time. The results are contained in the second column of @tbl-modelsummaryfivekmonly. Notice that the slope estimate is unchanged.

Following @gelmanhillvehtari2020 [p. 84] we recommend considering the coefficients as comparisons, rather than effects. And to use language that makes it clear these are comparisons, on average, based on one dataset. For instance, we may consider that the coefficient on the five-kilometer run time shows how different individuals compare. When comparing the marathon times of individuals in our dataset whose five-kilometer run time differed by one minute, on average we find their marathon times differs by about eight minutes. This makes sense seeing as a marathon is roughly that many times longer than a five-kilometer run.

We use `augment()` from `broom` [@broom] to add the fitted values and residuals to our original dataset. This allows us to plot the residuals (@fig-fivekmvsmarathonresids).

```{r}
sim_run_data <-
  broom::augment(
    sim_run_data_first_model,
    data = sim_run_data
  )
```

```{r}
#| eval: true
#| include: true
#| message: false
#| warning: false
#| label: fig-fivekmvsmarathonresids
#| layout-ncol: 2
#| fig-cap: "Residuals from the simple linear regression with simulated data on the time someone takes to run five kilometers and a marathon"
#| fig-subcap: ["Distribution of residuals", "Residuals by five-kilometer time", "Residuals by marathon time", "Comparing the estimated time with the actual time"]

ggplot(
  sim_run_data,
  aes(x = .resid)
) +
  geom_histogram(binwidth = 1) +
  theme_classic() +
  labs(
    y = "Number of occurrences",
    x = "Residuals"
  )

ggplot(
  sim_run_data,
  aes(x = five_km_time, y = .resid)
) +
  geom_point() +
  geom_hline(
    yintercept = 0,
    linetype = "dotted",
    color = "grey"
  ) +
  theme_classic() +
  labs(
    y = "Residuals",
    x = "Five-kilometer time (minutes)"
  )

ggplot(
  sim_run_data,
  aes(x = marathon_time, y = .resid)
) +
  geom_point() +
  geom_hline(
    yintercept = 0,
    linetype = "dotted",
    color = "grey"
  ) +
  theme_classic() +
  labs(
    y = "Residuals",
    x = "Marathon time (minutes)"
  )

ggplot(
  sim_run_data,
  aes(x = marathon_time, y = .fitted)) +
  geom_point() + 
  geom_abline(intercept = 0, slope = 1,
              linetype = "dashed",) +
  theme_classic() +
  labs(
    y = "Estimated marathon time",
    x = "Actual marathon time"
  )
```

<! -- really nice section here! -->

<!-- We want our estimate to be unbiased. When we say our estimate is unbiased, we are trying to say that even though for some sample our estimate might be too high, and for another sample our estimate might be too low, eventually if we have a lot of data then our estimate would be the same as the population. That is, an estimator is unbiased if it does not systematically over- or under-estimate [@islr, p. 65]. -->

We want to try to speak to the "true" relationship, so we need to try to capture how much we think our understanding depends on the sample that we have to analyze. And this is where the standard error comes in. It tells us how sure we can be about the estimates of parameters based on the data that we have (@fig-fivekmvsmarathon-3). Part of this is captured by the fact that standard errors are a function of sample size $n$, and as the sample size increases, the standard errors decrease.

From standard errors, we can compute a confidence interval. A 95 per cent confidence interval is a range, such that there is "approximately a 95 per cent chance that the interval" contains the population parameter, which is typically unknown [@islr, p. 66]. The lower end of this range is about: $\hat{\beta_1} - 2 \times \mbox{SE}\left(\hat{\beta_1}\right)$ and the upper end of this range is about: $\hat{\beta_1} + 2 \times \mbox{SE}\left(\hat{\beta_1}\right)$. For instance, in the case of the marathon time example, the lower end is $8.2 - 2 \times 0.3 = 7.6$ and the upper end is $8.2 + 2 \times 0.3 = 8.8$, and the true value (which we only know in this case because we simulated it) is 8.4.

We can use this machinery to test claims. For instance, we could claim that there is no relationship between $X$ and $Y$, i.e. $\beta_1 = 0$, as an alternative to a claim that there is some relationship between $X$ and $Y$, i.e. $\beta_1 \neq 0$. In @sec-hunt-data we needed to decide how much evidence it would take to convince us that our tea taster could distinguish whether milk or tea had been added first. In the same way, here we need to decide if the estimate of $\beta_1$, which we denote $\hat{\beta}_1$, is far enough away from zero for us to be comfortable claiming that $\beta_1 \neq 0$. If we were very confident in our estimate of $\beta_1$ then it would not have to be far, but if we were not, then it would have to be substantial. For instance, if the confidence interval contains zero, then we lack evidence to suggest $\beta_1 \neq 0$. The standard error of $\hat{\beta}_1$ does an awful lot of work here in accounting for a variety of factors, only some of which it can actually account for, as does our choice as to what it would take to convince us. 

We compare this standard error with $\hat{\beta}_1$ to get the "test statistic" or t-statistic:
$$t = \frac{\hat{\beta}_1}{\mbox{SE}(\hat{\beta}_1)}.$$ 
And we then compare our t-statistic to the t-distribution to compute the probability of getting this absolute t-statistic or a larger one, if it was the case that $\beta_1 = 0$. This probability is the p-value. A smaller p-value means there is a smaller "probability of observing something at least as extreme as the observed test statistic" [@gelmanhillvehtari2020, p. 57].

> Words! Mere words! How terrible they were! How clear, and vivid, and cruel! One could not escape from them. And yet what a subtle magic there was in them! They seemed to be able to give a plastic form to formless things, and to have a music of their own as sweet as that of viol or of lute. Mere words! Was there anything so real as words?
>
> *The Picture of Dorian Gray* [@wilde].

We will not make much use of p-values in this book because they are a specific and subtle concept. They are difficult to understand and easy to abuse. Even though they are "little help" for "scientific inference" many disciplines are incorrectly fixated on them [@nelderdoesntmiss, p. 257]. One issue is that they embody every assumption of the workflow, including everything that went into gathering and cleaning the data. While p-values have implications if all the assumptions were correct; when we consider the full data science workflow there are usually an awful lot of assumptions. And we do not get guidance from p-values about whether the assumptions are reasonable [@greenland2016statistical p. 339]. 

A p-value may reject a null hypothesis because the null hypothesis is false, but it may also be that some data were incorrectly gathered or prepared. We can only be sure that the p-value speaks to the hypothesis we are interested in testing, if all the other assumptions are correct. There is nothing inherently wrong about using p-values, but it is important to use them in sophisticated and thoughtful ways. @coxtalks discusses what this requires.

One application where it is easy to see an inappropriate focus on p-values is in power analysis. Power, in a statistical sense, refers to probability of rejecting a null hypothesis that is false. As power relates to hypothesis testing, it also relates to sample size. There is often a worry that a study is "under-powered", meaning there was not a large enough sample, but rarely a worry that, say, the data were inappropriately cleaned or imputed, even though we cannot distinguish between these based only on a p-value. As @meng2018statistical and @Bradley2021 demonstrate, a focus on power can blind us from our responsibility to ensure our data are high-quality.

:::{.callout-note}
## Shoulders of giants

Dr Nancy Reid is University Professor in the Department of Statistical Sciences at the University of Toronto. After obtaining a PhD in Statistics from Stanford University in 1979, she took a position as a postdoctoral fellow at Imperial College London. She was then appointed as assistant professor at the University of British Columbia in 1980, and then moved to the University of Toronto in 1986, where she was promoted to full professor in 1988 and served as department chair between 1997 and 2002 [@Staicu2017]. Her research focuses on obtaining accurate inference in small-sample regimes and developing inferential procedures for complex models featuring intractable likelihoods. @cox1987parameter examines how re-parameterizing models can simplify inference, @varin2011overview surveys methods for approximating intractable likelihoods, and @reid2003asymptotics overviews inferential procedures in the small-sample regime. Dr Reid was awarded the 1992 COPSS Presidents' Award, the Royal Statistical Society Guy Medal in Silver in 2016 and in Gold in 2022, and the COPSS Distinguished Achievement Award and Lectureship in 2022.
:::

## Multiple linear regression

To this point we have just considered one explanatory variable. But we will usually have more than one. One approach would be to run separate regressions for each explanatory variable. But compared with separate linear regressions for each, adding more explanatory variables allows for associations between the dependent variable and the independent variable of interest to be assessed while adjusting for other explanatory variables. Often the results will be quite different.

We may also like to consider explanatory variables that are not continuous. For instance: pregnant or not; day or night. When there are only two options then we can use a binary variable, which is considered either 0 or 1. If we have a column of character values that only has two values, such as: `c("Myles", "Ruth", "Ruth", "Myles", "Myles", "Ruth")`, then using this as an explanatory variable in the usual regression set up would mean that it is treated as a binary variable. If there are more than two levels, then we can use a combination of binary variables, where some baseline outcome gets integrated into the intercept.

As an example, we add whether it was raining to our simulated relationship between marathon and five-kilometer run times. We then specify that if it was raining, the individual is five minutes slower than they otherwise would have been.

```{r}
minutes_slower_in_rain <- 10

sim_run_data <-
  sim_run_data |>
  mutate(was_raining = sample(
    c("Yes", "No"),
    size = number_of_observations,
    replace = TRUE,
    prob = c(0.2, 0.8)
  )) |>
  mutate(
    marathon_time = if_else(
      was_raining == "Yes",
      marathon_time + minutes_slower_in_rain,
      marathon_time
    )
  ) |>
  select(five_km_time, marathon_time, was_raining)

sim_run_data
```

We can add additional explanatory variables to `lm()` with `+`. Again, we will include a variety of quick tests for class and the number of observations, and add another about missing values. We may not have any idea what the coefficient of rain should be, but if we did not expect it to make them faster, then we could also add a test of that with a wide interval of non-negative values.

```{r}
stopifnot(
  class(sim_run_data$marathon_time) == "numeric",
  class(sim_run_data$five_km_time) == "numeric",
  class(sim_run_data$was_raining) == "character",
  all(complete.cases(sim_run_data)),
  nrow(sim_run_data) == 200
)

sim_run_data_rain_model <-
  lm(
    marathon_time ~ five_km_time + was_raining,
    data = sim_run_data
  )

stopifnot(between(
  sim_run_data_rain_model$coefficients[3],
  0,
  20
))

summary(sim_run_data_rain_model)

```

The result, in the second column of @tbl-modelsummaryruntimes, shows that when we compare individuals in our dataset who ran in the rain with those who did not, the ones in the rain tended to have a slower time. And this corresponds with what we expect if we look at a plot of the data (@fig-fivekmvsmarathonbinary-1).

We have included two types of tests here. The ones run before `lm()` check inputs, and the ones run after `lm()` check outputs. We may notice that some of the input checks are the same as earlier. One way to avoid having to re-write tests many times, would be to use `testthat` [@testthat] to create a suite of tests of class in say an R file called "class_tests.R", which are then called using `test_file()`.

For instance, we could save the following as "test_class.R" in a dedicated tests folder.

```{r}
#| eval: false

test_that("Check class", {
  expect_type(sim_run_data$marathon_time, "double")
  expect_type(sim_run_data$five_km_time, "double")
  expect_type(sim_run_data$was_raining, "character")
})
```

We could save the following as "test_observations.R" 

```{r}
#| eval: false

test_that("Check number of observations is correct", {
  expect_equal(nrow(sim_run_data), 200)
})

test_that("Check complete", {
  expect_true(all(complete.cases(sim_run_data)))
})
```

And finally, we could save the following as "test_coefficient_estimates.R".

```{r}
#| eval: false

test_that("Check coefficients", {
  expect_gt(sim_run_data_rain_model$coefficients[3], 0)
  expect_lt(sim_run_data_rain_model$coefficients[3], 20)
})

```


We could then change the regression code to call these test files rather than write them all out.

```{r}
#| message: false

library(testthat)

test_file("tests/test_observations.R")
test_file("tests/test_class.R")

sim_run_data_rain_model <-
  lm(
    marathon_time ~ five_km_time + was_raining,
    data = sim_run_data
  )

test_file("tests/test_coefficient_estimates.R")
```

It is important to be clear about what we are looking for in the checks of the coefficients. When we simulate data, we put in place reasonable guesses for what the data could look like, and it is similarly reasonable guesses that we test. A failing test is not necessarily a reason to go back and change the data or the model, but instead a reminder to look at what is going on in both, and potentially update the test if necessary.

In addition to wanting to include additional explanatory variables, we may think that they are related to each another. For instance, maybe rain really matters if it is also humid that day. We are interested in the humidity and temperature, but also how those two variables interact (@fig-fivekmvsmarathonbinary-2). We can do this by using `*` instead of `+` when we specify the model. When we interact variables in this way, then we almost always need to include the individual variables as well and `lm()` will do this by default. The result is contained in the third column of @tbl-modelsummaryruntimes.

```{r}
minutes_slower_in_high_humidity <- 15

sim_run_data <-
  sim_run_data |>
  mutate(humidity = sample(
    c("High", "Low"),
    size = number_of_observations,
    replace = TRUE,
    prob = c(0.2, 0.8)
  )) |>
  mutate(
    marathon_time = if_else(
      humidity == "High",
      marathon_time + minutes_slower_in_high_humidity,
      marathon_time
    ),
    weather_conditions =
      case_when(
        was_raining == "No" & humidity == "Low" ~ "No rain, not humid",
        was_raining == "Yes" & humidity == "Low" ~ "Rain, not humid",
        was_raining == "No" & humidity == "High" ~ "No rain, humid",
        was_raining == "Yes" & humidity == "High" ~ "Rain, humid"
      )
  )

sim_run_data
```


```{r}
#| eval: true
#| fig-cap: "Simple linear regression with simulated data on the time someone takes to run five kilometers and a marathon, depending on the weather"
#| include: true
#| label: fig-fivekmvsmarathonbinary
#| message: false
#| warning: false
#| fig-subcap: ["Only whether it was raining", "Whether it was raining and the level of humidity"]
#| layout-ncol: 2

base <-
  sim_run_data |>
  ggplot(aes(x = five_km_time, y = marathon_time)) +
  labs(x = "Five-kilometer time (minutes)",
       y = "Marathon time (minutes)") +
  theme_classic() +
  scale_color_brewer(palette = "Set1") +
  theme(legend.position = "bottom")

base +
  geom_point(aes(color = was_raining)) +
  geom_smooth(
    aes(color = was_raining),
    method = "lm",
    alpha = 0.3,
    linetype = "dashed",
    formula = "y ~ x"
  ) +
  labs(color = "Was raining")

base +
  geom_point(aes(color = weather_conditions)) +
  geom_smooth(
    aes(color = weather_conditions),
    method = "lm",
    alpha = 0.3,
    linetype = "dashed",
    formula = "y ~ x"
  ) +
  labs(color = "Conditions")
```

```{r}
#| include: true
#| label: tbl-modelsummaryruntimes
#| tbl-cap: "Explaining marathon times based on five-kilometer run times and weather features"

sim_run_data_rain_and_humidity_model <-
  lm(
    marathon_time ~ five_km_time + was_raining * humidity,
    data = sim_run_data
  )

modelsummary::modelsummary(
  list(
    "Five km only" = sim_run_data_first_model,
    "Add rain" = sim_run_data_rain_model,
    "Add humidity" = sim_run_data_rain_and_humidity_model
  ),
  fmt = 2,
  statistic = "conf.int"
)
```

<! -- below mini para is a little loose be specific on: "and usually graphs and associated text are sufficient" -->

There are a variety of threats to the validity of linear regression estimates, and aspects to think about, particularly when using an unfamiliar dataset. We need to address these when we use it, and usually graphs and associated text are sufficient to assuage most of them. Aspects of concern include:

1. Linearity of explanatory variables. We are concerned with whether the independent variables enter in a linear way. We can usually be convinced there is enough linearity in our explanatory variables for our purposes by using graphs of the variables. <! -- i thought this was linearity of conditional relationship, which could also be violated if dep var is not linear? -->
2. Independence of errors. We are concerned that the errors are not correlated with each other. For instance, if we are interested in weather-related measurement such as average daily temperature, then we may find a pattern because the temperature on one day is likely similar to the temperature on another. We can be convinced that we have satisfied this condition by making graphs of estimated compared with actual outcomes, such as @fig-fivekmvsmarathonresids-4, or graphs of the residuals compared with observed values, such as @fig-fivekmvsmarathonresids-3.
3. Homoscedasticity of errors. We are concerned that the errors are not becoming systematically larger or smaller throughout the sample. If that is happening, then we term it heteroscedasticity. Again, graphs of errors, such as @fig-fivekmvsmarathonresids-2, are used to convince us of this.
<!-- 4. Normality of errors. We are concerned that our errors are normally distributed when we are interested in making individual-level predictions. -->
4. Outliers and other high-impact observations. Finally, we might be worried that our results are being driven by a handful of observations. For instance, thinking back to @sec-static-communication and Anscombe's Quartet, we notice that linear regression estimates would be heavily influenced by the inclusion of one or two particular points. We can become comfortable with this by considering our analysis on various sub-sets.

<! -- i usually teach LINE linearity, independence, normality of resids, and equal variance (homosked) + outliers at end and looks like you've deleted one of assumptions here. is there good reason for it, like the relationship of residuals to linearity? -->

Those aspects are statistical concerns and relate to whether the model is working. The most important threat to validity and hence the aspect that must be addressed at some length, is whether or not this model is directly relevant to the research question of interest.

## Two cultures

@breiman2001statistical describes two cultures of statistical modelling: one focused on prediction and the other on inference. In general, around this time, various disciplines tended to focus on either inference or prediction. The rise of data science has meant there is now a need to be comfortable with both [@Neufeld2021]. The two cultures are being brought closer together, and there is an overlap and interaction between prediction and inference. But their separate evolution means there are still considerable cultural differences.

In this book we use frequentist GLMs within `tidymodels` when we are interested in prediction. And when we are focused on inference, instead, we use the probabilistic programming language Stan to fit GLMs in a Bayesian framework, and interface with it using `rstanarm` [@citerstanarm]. This is slightly simplistic---we could do prediction with Bayesian models, and we could use Stan <! -- did you mean causal inference here? --> 
within `tidymodels`. But, for the sake of simplicity and clarity, we keep these separated in this way for this book. It is important to be conversant in both ecosystems and cultures.

<! -- i am not entirely sure why the distinction between frequentist/bayes for prediction/inference respectively. if anything i would expect the other way around, but i know that's mostly a conventional thing. at the moment this passage still reads a little like you want students to do things this way. maybe just say it's entirely arbitrary, do what makes you feel warm inside kids! --> 

### Prediction

When we are focused on prediction, we will often want to fit many models. One way to do this is to copy and paste code many times. This is okay, and it is the way that most people get started but it is prone to making errors that are hard to find. A better approach will: 

1) scale more easily; 
2) enable us to think carefully about over-fitting; and 
3) add model evaluation.

The use of `tidymodels` [@citeTidymodels] satisfies these criteria by providing a coherent grammar that allows us to easily fit a variety of models. Like the `tidyverse`, it is a package of packages.

By way of illustration, we want to estimate the following model for the simulated running data:

$$
\begin{aligned}
y_i | \mu_i &\sim \mbox{Normal}(\mu_i, \sigma) \\
\mu_i &= \beta_0 +\beta_1x_i
\end{aligned}
$$
where $y_i$ refers to the marathon time of some individual $i$ and $x_i$ refers to their five-kilometer time. Here we say that the marathon time of some individual $i$ is normally distributed with a mean of $\mu$ and a standard deviation of $\sigma$, where the mean depends on two parameters $\beta_0$ and $\beta_1$ and their five-kilometer time. Here "~" means "is distributed as". We use this slightly different notation from earlier to be more explicit about the distributions being used, but this model is equivalent to $y_i=\beta_0+\beta_1 x_i + \epsilon_i$, where $\epsilon$ is normally distributed.

As we are focused on prediction, we are worried about over-fitting our data, which would limit our ability to make claims about other datasets. One way to partially address this is to split our dataset in two using `initial_split()`. 

```{r}
#| message: false
#| warning: false

library(tidymodels)

set.seed(853)

sim_run_data_split <-
  initial_split(
    data = sim_run_data,
    prop = 0.80
  )

sim_run_data_split
```

Having split the data, we then create training and test datasets with `training()` and `testing()`.

```{r}
sim_run_data_train <- training(sim_run_data_split)

sim_run_data_test <- testing(sim_run_data_split)
```

We have placed 80 per cent of our dataset into the training dataset. We will use that to estimate the parameters of our model. We have kept the remaining 20 per cent of it back, and we will use that to evaluate our model. Why might we do this? Our concern is the bias-variance trade-off, which haunts all aspects of modelling. We are concerned that our results may be too particular to the dataset that we have, such that they are not applicable to other datasets. To take an extreme example, consider a dataset with ten observations. We could come up with a model that perfectly hits those observations. But when we took that model to other datasets, even those generated by the same underlying process, it would not be accurate.

One way to deal with this concern is to split the data in this way. We use the training data to inform our estimates of the coefficients, and then using the test data, to evaluate the model. A model that too closely matched the data in the training data would not do well in the test data, because it would be too specific to the training data. The use of this test-training split enables us the opportunity to build an appropriate model.

It is more difficult to do this separation appropriately than one might initially think. We want to avoid the situation where aspects of the test dataset are present in the training dataset because this inappropriately telegraphs what is about to happen. This is called data leakage. But if we consider data cleaning and preparation, which likely involves the entire dataset, it may be that some features of each are influencing each other. @kapoornarayanan2022 find extensive data leakage in applications of machine learning that could invalidate much research.

:::{.callout-note}
## Shoulders of giants

Dr Daniela Witten is the Dorothy Gilford Endowed Chair of Mathematical Statistics and Professor of Statistics & Biostatistics at the University of Washington. After taking a PhD in Statistics from Stanford University in 2010, she joined the University of Washington as an assistant professor. She was promoted to full professor in 2018. One active area of her research is double-dipping which is focused on the effect of sample splitting [@selectiveinference]. She is an author of the influential @islr. She was appointed a Fellow of the American Statistical Association in 2020 and awarded the COPSS Presidents' Award in 2022.
:::

To use `tidymodels` we first specify that we are interested in linear regression with `linear_reg()`. We then specify the type of linear regression, in this case multiple linear regression, with `set_engine()`. Finally, we specify the model with `fit()`. While this is considerable more infrastructure than the base R approach detailed above, it advantage of this approach is that it can be used to fit many models. 

```{r}
sim_run_data_first_model_tidymodels <-
  linear_reg() |>
  set_engine(engine = "lm") |>
  fit(
    marathon_time ~ five_km_time + was_raining,
    data = sim_run_data_train
  )
```

The estimated coefficients are summarized in the first column of @tbl-modelsummarybayesbetter. For instance, we find that on average in our dataset, five-kilometer run times that are one minute longer are associated with marathon times that are about 8 minutes longer.

### Inference

Running a regression in a Bayesian setting is similar to frequentist (i.e. `lm()`, `glm()`, etc). The main difference from a regression point of view, is that the parameters involved in the models (i.e. $\beta_0$, $\beta_1$, etc) are considered random variables, and so themselves have associated probability distributions. 

Before we run a regression in a Bayesian framework, we need to decide on a starting probability distribution for each of these parameters, which we call a "prior". This is another reason for the workflow advocated in this book: the simulation stage leads directly to priors. We will again specify the model that we are interested in, but this time we include priors. <! -- this bit feels inevitably a little like drinking from the firehose! i am not sure if any of the required readings cover priors in depth. do they? if you need cut to provide space, there's a long ish passage about why p-values aren't the best, would be better spent explaining what priors are and why they are good imo--> 


$$
\begin{aligned}
y_i|\mu_i, \sigma &\sim \mbox{Normal}(\mu_i, \sigma) \\
\mu_i &= \beta_0 +\beta_1x_i\\
\beta_0 &\sim \mbox{Normal}(0, 2.5) \\
\beta_1 &\sim \mbox{Normal}(0, 2.5) \\
\sigma &\sim \mbox{Exponential}(1) \\
\end{aligned}
$$

We then combine information from the data with the priors to obtain posterior distributions for our parameters. Inference is then carried out based on analyzing posterior distributions.

Another aspect that is different between Bayesian approaches and the way we have been doing modelling to this point, is that Bayesian models will usually take longer to run. Because of this, it can be useful to run the model, either within the Quarto document or in a separate R script, and then save it with `saveRDS()`. With sensible Quarto chunk options for "eval" and "echo" (see @sec-reproducible-workflows), the model can then be read into the Quarto document with `readRDS()` rather than being run every time the paper is compiled. In this way, the model delay is only imposed once for a given model. It can also be useful to add `beep()` from `beepr` [@beepr] to the end of the model, to get an audio notification when the model is done.

```{r}
#| echo: true
#| eval: false
#| message: false
#| warning: false

library(rstanarm)

sim_run_data_first_model_rstanarm <-
  stan_glm(
    formula = marathon_time ~ five_km_time + was_raining,
    data = sim_run_data,
    family = gaussian(),
    prior = normal(location = 0, scale = 2.5),
    prior_intercept = normal(location = 0, scale = 2.5),
    prior_aux = exponential(rate = 1),
    seed = 853
  )

beepr::beep()

saveRDS(
  sim_run_data_first_model_rstanarm,
  file = "sim_run_data_first_model_rstanarm.rds"
)
```

```{r}
#| echo: false
#| eval: false
#| message: false
#| warning: false

# INTERNAL
saveRDS(
  sim_run_data_first_model_rstanarm,
  file = "outputs/model/sim_run_data_first_model_rstanarm.rds"
)
```

```{r}
#| echo: true
#| eval: false
#| warning: false
#| message: false

sim_run_data_first_model_rstanarm <-
  readRDS(file = "sim_run_data_first_model_rstanarm.rds")
```

```{r}
#| echo: false
#| eval: true
#| warning: false
#| message: false

library(rstanarm)

sim_run_data_first_model_rstanarm <-
  readRDS(file = "outputs/model/sim_run_data_first_model_rstanarm.rds")
```

We use `stan_glm()` with the "gaussian()" family to specify multiple linear regression and the model formula is written in the same way as base R and `tidymodels`. We have explicitly added the default priors, although strictly this is not necessary.

The estimation results, which are in the second column of @tbl-modelsummarybayesbetter, are not quite what we expect. For instance, the rate of increase in marathon times is estimated to be around three minutes per minute of increase in five-kilometer time, which seems low given the ratio of a five-kilometer run to marathon distance.

The issue of picking priors is a challenging one and the subject of extensive research. For the purposes of this book, using the `rstanarm` defaults is fine. But even if they are just the default, priors should be explicitly specified in the model and included in the function. This is to make it clear to others what has been done. We could use `default_prior_intercept()` and `default_prior_coef()` to find the default priors in `rstanarm` and then explicitly include them in the model. 

It is normal to find it difficult to know what prior to specify. Getting started by adapting someone else's `rstanarm` code is perfectly fine. If they have not specified their priors, then we can use the helper function `prior_summary()`, to find out which priors were used.

```{r}
prior_summary(sim_run_data_first_model_rstanarm)
```

We are interested in understanding what the priors imply before we involve any data. We do this by implementing prior predictive checks. This means simulating from the priors to look at what the model implies about the possible magnitude and direction of the relationships between the explanatory and outcome variables. This process is no different to all the other simulation that we have done to this point.

```{r}
number_draws <- 1000

priors <-
  tibble(
    sigma = rep(rexp(n = number_draws, rate = 1), times = 16),
    beta_0 = rep(rnorm(n = number_draws, mean = 0, sd = 2.5), times = 16),
    beta_1 = rep(rnorm(n = number_draws, mean = 0, sd = 2.5), times = 16),
    five_km_time = rep(15:30, each = number_draws),
    mu = beta_0 + beta_1 * five_km_time
  ) |>
  rowwise() |>
  mutate(
    marathon_time = rnorm(n = 1, mean = mu, sd = sigma)
  )
```

```{r}
#| echo: true
#| eval: true
#| message: false
#| warning: false
#| label: fig-worstmodelever
#| fig-cap: "Some implications from the priors that were used"
#| layout-ncol: 2
#| fig-subcap: ["Distribution of implied marathon times", "Relationship between 5km and marathon times"]

priors |>
  ggplot(aes(x = marathon_time)) +
  geom_histogram(binwidth = 10) +
  theme_classic()

priors |>
  ggplot(aes(x = five_km_time, y = marathon_time)) +
  geom_point(alpha = 0.1) +
  theme_classic()
```

@fig-worstmodelever suggests our model has been poorly constructed. Not only are there world record marathon times, but there are also negative marathon times! One issue is that our prior for $\beta_1$ does not take in all the information that we know. We know that a marathon is about eight times longer than a five-kilometer run and so we could center the prior for $\beta_1$ around that. Our re-specified model is:

$$
\begin{aligned}
y_i|\mu_i, \sigma &\sim \mbox{Normal}(\mu_i, \sigma) \\
\mu_i &= \beta_0 +\beta_1x_i\\
\beta_0 &\sim \mbox{Normal}(0, 2.5) \\
\beta_1 &\sim \mbox{Normal}(8, 2.5) \\
\sigma &\sim \mbox{Exponential}(1) \\
\end{aligned}
$$
And we can see from prior prediction checks that it seems more reasonable (@fig-secondworstmodelever).

```{r}
number_draws <- 1000

updated_priors <-
  tibble(
    sigma = rep(rexp(n = number_draws, rate = 1), times = 16),
    beta_0 = rep(rnorm(n = number_draws, mean = 0, sd = 2.5), times = 16),
    beta_1 = rep(rnorm(n = number_draws, mean = 8, sd = 2.5), times = 16),
    five_km_time = rep(15:30, each = number_draws),
    mu = beta_0 + beta_1 * five_km_time
  ) |>
  rowwise() |>
  mutate(
    marathon_time = rnorm(n = 1, mean = mu, sd = sigma)
  )
```

```{r}
#| echo: true
#| eval: true
#| message: false
#| warning: false
#| fig-cap: "Updated priors"
#| label: fig-secondworstmodelever
#| layout-ncol: 2
#| fig-subcap: ["Distribution of implied marathon times", "Relationship between 5km and marathon times"]

updated_priors |>
  ggplot(aes(x = marathon_time)) +
  geom_histogram(binwidth = 10) +
  theme_classic()

updated_priors |>
  ggplot(aes(x = five_km_time, y = marathon_time)) +
  geom_point(alpha = 0.1) +
  theme_classic()
```

Additionally, `rstanarm` can help to improve the specified priors, by scaling them based on the data. Specify the prior that you think is reasonable and include it in the function, but also include "autoscale = TRUE", and `rstanarm` will adjust the scale. When we re-run our model with these updated priors and allowing auto-scaling we get much better results, which are in the third column of @tbl-modelsummarybayesbetter.

```{r}
#| include: true
#| message: false
#| warning: false
#| eval: false

sim_run_data_second_model_rstanarm <-
  stan_glm(
    formula = marathon_time ~ five_km_time + was_raining,
    data = sim_run_data,
    family = gaussian(),
    prior = normal(location = 8, scale = 2.5, autoscale = TRUE),
    prior_intercept = normal(0, 2.5, autoscale = TRUE),
    prior_aux = exponential(rate = 1, autoscale = TRUE),
    seed = 853
  )

saveRDS(
  sim_run_data_second_model_rstanarm,
  file = "sim_run_data_second_model_rstanarm.rds"
)
```

```{r}
#| include: false
#| message: false
#| warning: false
#| eval: false

# INTERNAL
saveRDS(
  sim_run_data_second_model_rstanarm,
  file = "outputs/model/sim_run_data_second_model_rstanarm.rds"
)
```

```{r}
#| eval: true
#| include: false
#| warning: false
#| message: false

library(rstanarm)

sim_run_data_second_model_rstanarm <-
  readRDS(file = "outputs/model/sim_run_data_second_model_rstanarm.rds")
```

```{r}
#| label: tbl-modelsummarybayesbetter
#| tbl-cap: "Forecasting and explanatory models of marathon times based on five-kilometer run times"
#| warning: false

modelsummary::modelsummary(
  list(
    "tidymodels" = sim_run_data_first_model_tidymodels,
    "rstanarm" = sim_run_data_first_model_rstanarm,
    "rstanarm better priors" = sim_run_data_second_model_rstanarm
  ),
  statistic = "conf.int"
)

```

As we used the "autoscale = TRUE" option, it can be helpful to look at how the priors were updated.

```{r}
prior_summary(sim_run_data_second_model_rstanarm)
```

Having built a Bayesian model, we may want to look at what it implies (@fig-ppcheckandposteriorvsprior). One way to do this is to consider the posterior distribution. 

One way to use the posterior distribution is to consider whether the model is doing a good job of fitting the data. The idea is that if the model is doing a good job of fitting the data, then the posterior should be able to be used to simulate data that are like the actual data [@bayesianworkflow]. We can implement a posterior predictive check with `pp_check()` (@fig-ppcheckandposteriorvsprior-1). This compares the actual outcome variable with simulations from the posterior distribution. And we can compare the posterior with the prior with `posterior_vs_prior()` to see how much the estimates change once data are taken into account  (@fig-ppcheckandposteriorvsprior-2). Helpfully, `pp_check()` and `posterior_vs_prior()` return `ggplot` objects so we can modify the look of them in the normal way we manipulate graphs. These checks and discussion would typically just be briefly mentioned in the main content of a paper, with all the detail and graphs added to a dedicated appendix.

```{r}
#| eval: true
#| echo: true
#| message: false
#| warning: false
#| label: fig-ppcheckandposteriorvsprior
#| layout-ncol: 2
#| fig-cap: "Examining how the model fits, and is affected by, the data"
#| fig-subcap: ["Posterior prediction check", "Comparing the posterior with the prior"]

pp_check(sim_run_data_second_model_rstanarm) +
  theme_classic() +
  theme(legend.position = "bottom")

posterior_vs_prior(sim_run_data_second_model_rstanarm) +
  theme_minimal() +
  scale_color_brewer(palette = "Set1") +
  theme(legend.position = "bottom") +
  coord_flip()
```

With a simple model like this, the differences between the predict and inference approaches are minimal. But as model or data complexity increases these differences can become important.

We have already discussed confidence intervals and the Bayesian equivalent to a confidence interval is called a "credibility interval", and reflects two points such there is a certain probability mass between them, in this case 95 per cent. @tbl-modelsummarybayesbetter shows confidence intervals for `tidymodels` and credible intervals for `rstanarm`, beneath the estimates. Bayesian estimation provides a distribution for each coefficient. This means there are actually an infinite number of points that we could use to generate this interval. We will include credible interval points in tables, because this is commonly needed, but it is important that the entire distribution is shown graphically. This might be using a cross-referenced appendix.

```{r}
plot(sim_run_data_second_model_rstanarm, 
     "areas")
```

The final aspect that we would like to check is a practical matter. `rstanarm` uses a sampling algorithm called "Markov chain Monte Carlo" (MCMC) to obtain samples from the posterior distributions of interest. We need to quickly check for signs that the algorithm ran into issues. In particular, we consider a trace plot, such as @fig-stanareyouokay-1, and an Rhat plot, such as @fig-stanareyouokay-2. These would typically go in a cross-referenced appendix.

```{r}
#| echo: true
#| eval: true
#| message: false
#| warning: false
#| label: fig-stanareyouokay
#| fig-cap: "Checking the convergence of the MCMC algorithm"
#| fig-subcap: ["Trace plot", "Rhat plot"]
#| layout-ncol: 2

plot(sim_run_data_second_model_rstanarm, "trace")

plot(sim_run_data_second_model_rstanarm, "rhat")
```

In the trace plot, we are looking for lines that appear to bounce around, but are horizontal, and have a nice overlap between the chains. The trace plot in @fig-stanareyouokay-1 does not suggest anything out of the ordinary. Similarly, with the Rhat plot, we are looking for everything to be close to 1, and certainly no more than 1.1. Again @fig-stanareyouokay-2 is an example that does not suggest any problems. If these diagnostics do not seem similar to this, then simplify the model and re-run it.

## Logistic regression

Linear regression is a useful way to better understand our data. But it assumes a continuous outcome variable that can take any number on the real line. We would like some way to use this same machinery when we cannot satisfy this condition. We turn to logistic and Poisson regression for binary and count outcome variables, respectively, noting they are still linear models, because the independent variables enter in a linear fashion.

Logistic regression, and its close variants, are useful in a variety of settings, from elections [@wang2015forecasting] through to horse racing [@chellel2018gambler; @boltonruth]. We use logistic regression when the dependent variable is a binary outcome, such as 0 or 1, or "yes" or "no". Although the presence of a binary outcome variable may sound limiting, there are a lot of circumstances in which the outcome either naturally falls into this situation or can be adjusted into it.

The foundation of this is the Bernoulli distribution where there is a certain probability, $p$, of outcome "1" and the remainder, $1-p$, for outcome "0". We can use `rbernoulli()` to simulate data from the Bernoulli distribution. 

```{r}
#| message: false
#| warning: false

set.seed(853)

Bernoulli_example <-
  tibble(draws = rbernoulli(n = 20, p = 0.1))

Bernoulli_example |> pull(draws)
```

One reason to use logistic regression is that we will be modelling a probability and so it will be bounded between 0 and 1. Whereas with linear regression we may end up with values outside this. The foundation of logistic regression is the logit function:

$$
\mbox{logit}(x) = \log\left(\frac{x}{1-x}\right).
$$
This will transpose values between 0 and 1, onto the real line. For instance, `logit(0.1) = -2.2`, `logit(0.5) = 0`, and `logit(0.9) = 2.2`.

To illustrate logistic regression, we will simulate data on whether it is a weekday or weekend, based on the number of cars on the road. We will assume that on weekdays the road is busier.

```{r}
#| message: false
#| warning: false

set.seed(853)

week_or_weekday <-
  tibble(
    number_of_cars = runif(n = 1000, min = 0, 100),
    noise = rnorm(n = 1000, mean = 0, sd = 10),
    is_weekday = if_else(number_of_cars + noise > 50, 1, 0)
  ) |>
  mutate(number_of_cars = round(number_of_cars)) |>
  select(-noise)

week_or_weekday
```
<!-- really good logit section without getting too into the MLE stuff -->

We can use `glm()` from base R to do a quick estimation. In this case we will try to work out whether it is a weekday or weekend, based on the number of cars we can see. We are interested in estimating @eq-logisticexample:
$$
\mbox{Pr}(y_i=1) = \mbox{logit}^{-1}\left(\beta_0+\beta_1 x_i\right)
$$ {#eq-logisticexample}
where $y_i$ is whether it is a weekday and $x_i$ is the number of cars on the road.


```{r}
week_or_weekday_model <-
  glm(
    is_weekday ~ number_of_cars,
    data = week_or_weekday,
    family = "binomial"
  )
```

The results of the estimation are in the first column of @tbl-modelsummarylogistic. The estimated coefficient on the number of cars is 0.19.

One reason that logistic regression can be a bit tricky initially is because the coefficients take a bit of work to interpret. The estimate of 0.19 is the average change in the log odds of it being a weekday with observing one extra car on the road. The coefficient is positive which means an increase, but as it is non-linear, if we want to specify a particular change, then this will be different for different observations.

We can translate our estimate into the probability of it being a weekday, for a given number of cars. `predictions()` from `marginaleffects` [@marginaleffects] can be used to add the implied probability that it is a weekday for each observation.

```{r}
library(marginaleffects)

week_or_weekday_predictions <-
  predictions(week_or_weekday_model) |>
  as_tibble()

week_or_weekday_predictions
```

And we can then graph the probability that our model implies, for each observation, of it being a weekday (@fig-dayornightprobs).

```{r}
#| eval: true
#| fig-cap: "Logistic regression probability results with simulated data of whether it is a weekday or weekend based on the number of cars that are around"
#| include: true
#| label: fig-dayornightprobs
#| message: false
#| warning: false

week_or_weekday_predictions |>
  mutate(is_weekday = factor(is_weekday)) |>
  ggplot(aes(
    x = number_of_cars,
    y = predicted,
    color = is_weekday
  )) +
  geom_jitter(width = 0.01, height = 0.01, alpha = 0.3) +
  labs(
    x = "Number of cars that were seen",
    y = "Estimated probability it is a weekday",
    color = "Was actually weekday"
  ) +
  theme_classic() +
  scale_color_brewer(palette = "Set1") +
  theme(legend.position = "bottom")
```

The marginal effect at each observation is of interest because it provides a sense of how this probability is changing. It enables us to say that at the mean (which in this case is if we were to see 50 cars) the probability of it being a weekday, increases by almost five per cent if we were to see another one (@tbl-marginaleffectcar).

```{r}
#| label: tbl-marginaleffectcar
#| tbl-cap: "Marginal effect of another car on the probability that it is a weekday, at the mean"

marginaleffects(week_or_weekday_model, newdata = "mean") |> 
  select(term, dydx, std.error) |> 
  knitr::kable(
    col.names = c("Term", "Slope", "Standard error")
  )
```

We could use `tidymodels` to run this if we wanted. To do that, we first need to change the class of our dependent variable into a factor because this is required for classification models.

```{r}
set.seed(853)

week_or_weekday <-
  week_or_weekday |>
  mutate(is_weekday = as_factor(is_weekday))

week_or_weekday_split <- initial_split(week_or_weekday, prop = 0.80)
week_or_weekday_train <- training(week_or_weekday_split)
week_or_weekday_test <- testing(week_or_weekday_split)

week_or_weekday_tidymodels <-
  logistic_reg(mode = "classification") |>
  set_engine("glm") |>
  fit(
    is_weekday ~ number_of_cars,
    data = week_or_weekday_train
  )
```

As before, we can make a graph of the actual results compared with our estimates. But one nice aspect of this is that we could use our test dataset to evaluate our model's forecasting ability more thoroughly, for instance through a confusion matrix, which sets the prediction of the model against the truth from the original data. It is arranged in four cells: true positives (positives correctly predicted); true negatives (negatives correctly predicted); false positives (model predicts positive incorrectly); and false negatives (model predicts negatives incorrectly). We find that the model does well on the held-out dataset. There were 90 observations where the model predicted it was a weekday, and it was actually a weekday, and 95 where the model predicted it was a weekend, and it was a weekend. It was wrong for 15 observations, and these were split across seven where it forecast a weekday, but it was a weekend, and eight where it was the opposite case. <!-- confusion matrices are confusing!-->

```{r}
week_or_weekday_tidymodels |>
  predict(new_data = week_or_weekday_test) |>
  cbind(week_or_weekday_test) |>
  conf_mat(truth = is_weekday, estimate = .pred_class)
```

We might be interested in inference, and so want to build a Bayesian model using `rstanarm`. Again, we will specify priors for our model, but use the default priors that `rstanarm` uses:

$$
\begin{aligned}
y_i|\pi_i & \sim \mbox{Bern}(\pi_i) \\
\mbox{logit}(\pi_i) & = \left(\beta_0+\beta_1 x_i\right) \\
\beta_0 & \sim \mbox{Normal}(0, 2.5)\\
\beta_1 & \sim \mbox{Normal}(0, 2.5)
\end{aligned}
$$
where $y_i$ is whether it is a weekday (actually 0 or 1), $x_i$ is the number of cars on the road, and $\pi_i$ is the probability that observations $i$ is a weekday.

```{r}
#| eval: false
#| echo: true
#| message: false
#| warning: false

week_or_weekday_rstanarm <-
  stan_glm(
    is_weekday ~ number_of_cars,
    data = week_or_weekday,
    family = binomial(link = "logit"),
    prior = normal(location = 0, scale = 2.5, autoscale = TRUE),
    prior_intercept = normal(location = 0, scale = 2.5, autoscale = TRUE),
    seed = 853
  )

saveRDS(
  week_or_weekday_rstanarm,
  file = "week_or_weekday_rstanarm.rds"
)
```

```{r}
#| eval: false
#| include: false
#| message: false
#| warning: false

# INTERNAL
saveRDS(
  week_or_weekday_rstanarm,
  file = "outputs/model/week_or_weekday_rstanarm.rds"
)
```

```{r}
#| eval: true
#| include: false
#| message: false
#| warning: false

week_or_weekday_rstanarm <-
  readRDS(file = "outputs/model/week_or_weekday_rstanarm.rds")
```

We can compare our different models (@tbl-modelsummarylogistic).

```{r}
#| label: tbl-modelsummarylogistic
#| tbl-cap: "Forecasting and explanatory models of whether it is day or night, based on the number of cars on the road"

modelsummary::modelsummary(
  list(
    "base R" = week_or_weekday_model,
    "tidymodels" = week_or_weekday_tidymodels,
    "rstanarm" = week_or_weekday_rstanarm
  ),
  statistic = "conf.int"
)
```

@tbl-modelsummarylogistic makes it clear that each of the approaches is similar in this case. They agree on the direction of the effect of seeing an extra car on the probability of it being a weekday. And even the magnitude of the effect is estimated to be similar.


### US political support

One area where logistic regression is often used is political polling. In many cases voting implies the need for one preference ranking, and so issues are reduced, whether appropriately or not, to "support" or "not support".

As a reminder, the workflow we advocate in this book: Plan -> Simulate -> Acquire -> Explore -> Share. While the focus here is the exploration of data using linear models, we still need to do the other aspects. We begin by planning. In this case, we are interested in US political support. In particular we are interested in whether we can forecast who a respondent is likely to vote for, based only on knowing their highest level of education and gender. That means we are interested in a dataset with variables for who an individual voted for, and some of their characteristics, for instance, gender and education. A quick sketch of such a dataset is @fig-uspoliticalsupportsketch. And we would like our model to average over these points. A quick sketch is @fig-uspoliticalsupportmodel.

::: {#fig-uspoliticalsuppor layout-ncol=2 layout-valign="bottom"}

![Quick sketch of a dataset that could be used to examine US political support](figures/IMG_2054.png){#fig-uspoliticalsupportsketch}

![Quick sketch of what we expect from the analysis before finalizing either the data of the analysis](figures/IMG_2055.png){#fig-uspoliticalsupportmodel}

Sketches of the expected dataset and analysis focus and clarify our thinking even if they will be updated later
:::

We will simulate a dataset where the chance that a person supports Biden depends on their gender and education.

```{r}
number_of_observations <- 1000

us_political_preferences <-
  tibble(
    education = sample(1:5, size = number_of_observations, replace = TRUE),
    gender = sample(1:2, size = number_of_observations, replace = TRUE),
    support_prob = 1 - 1 / (education + gender)
  ) |>
  rowwise() |>
  mutate(supports_biden = sample(
    c("yes", "no"),
    size = 1,
    replace = TRUE,
    prob = c(support_prob, 1 - support_prob)
  )) |>
  ungroup() |>
  select(-support_prob, supports_biden, gender, education)

us_political_preferences
```

We can use the 2020 Cooperative Election Study (CES) [@cooperativeelectionstudyus], which is a long-standing annual survey of US political opinion. In 2020, there were 61,000 respondents who completed the post-election survey. While the sampling methodology, detailed in @guidetothe2020ces [p. 13], is not a perfect random sample and relies on matching, it is widely used and accepted approach that balances sampling concerns and cost.

We can access the CES using `get_dataframe_by_name()` from `dataverse` [@dataverse], which was an approach that was introduced in @sec-gather-data and @sec-store-and-share. We save the data that are of interest to us, and then refer to that saved dataset.

```{r}
#| echo: true
#| eval: false

library(dataverse)

ces2020 <-
  get_dataframe_by_name(
    filename = "CES20_Common_OUTPUT_vv.csv",
    dataset = "10.7910/DVN/E9N6PH",
    server = "dataverse.harvard.edu",
    .f = readr::read_csv
  ) |>
  select(votereg, CC20_410, gender, educ)

write_csv(ces2020, "ces2020.csv")
```

```{r}
#| echo: false
#| eval: false

# INTERNAL

write_csv(ces2020, "inputs/data/ces2020.csv")
```

```{r}
#| echo: true
#| eval: false

ces2020 <-
  read_csv(
    "ces2020.csv",
    col_types =
      cols(
        "votereg" = col_integer(),
        "CC20_410" = col_integer(),
        "gender" = col_integer(),
        "educ" = col_integer()
      )
  )

ces2020
```

```{r}
#| echo: false
#| eval: true

# INTERNAL

ces2020 <-
  read_csv(
    "inputs/data/ces2020.csv",
    col_types =
      cols(
        "votereg" = col_integer(),
        "CC20_410" = col_integer(),
        "gender" = col_integer(),
        "educ" = col_integer()
      )
  )

ces2020
```

When we look at the actual data, there are concerns that we did not anticipate in our sketches. We only want respondents who are registered to vote, and we are only interested in those that voted for either Biden or Trump. We can filter to only those respondents and then add more informative labels. Gender of "female" and "male" is what is available from the CES.

```{r}
ces2020 <-
  ces2020 |>
  filter(
    votereg == 1,
    CC20_410 %in% c(1, 2)
  ) |>
  mutate(
    voted_for = if_else(CC20_410 == 1, "Biden", "Trump"),
    voted_for = as_factor(voted_for),
    gender = if_else(gender == 1, "Male", "Female"),
    education = case_when(
      educ == 1 ~ "No HS",
      educ == 2 ~ "High school graduate",
      educ == 3 ~ "Some college",
      educ == 4 ~ "2-year",
      educ == 5 ~ "4-year",
      educ == 6 ~ "Post-grad"
    ),
    education = factor(
      education,
      levels = c(
        "No HS",
        "High school graduate",
        "Some college",
        "2-year",
        "4-year",
        "Post-grad"
      )
    )
  ) |>
  select(voted_for, gender, education)
```

In the end we are left with 43,554 respondents (@fig-cesissogooditslikecheating). 

```{r}
#| eval: true
#| echo: true
#| message: false
#| warning: false
#| fig-cap: "The distribution of presidential preferences, by gender, and highest education"
#| label: fig-cesissogooditslikecheating

ces2020 |>
  ggplot(aes(x = education, fill = voted_for)) +
  stat_count(position = "dodge") +
  facet_wrap(facets = vars(gender)) +
  theme_minimal() +
  labs(
    x = "Highest education",
    y = "Number of respondents",
    fill = "Voted for"
  ) +
  coord_flip() +
  scale_fill_brewer(palette = "Set1") +
  theme(legend.position = "bottom")
```

One approach is to use `tidymodels` to build a prediction-focused logistic regression model in the same way as before i.e. a validation set approach [@islr, p. 176]. In this case, the probability will be that of voting for Biden.

```{r}
set.seed(853)

ces2020_split <- initial_split(ces2020, prop = 0.80)
ces2020_train <- training(ces2020_split)
ces2020_test <- testing(ces2020_split)

ces_tidymodels <-
  logistic_reg(mode = "classification") |>
  set_engine("glm") |>
  fit(
    voted_for ~ gender + education,
    data = ces2020_train
  )

ces_tidymodels
```

And then evaluate it on the test set. It appears as though the model is having difficulty identifying Trump supporters.

```{r}
ces_tidymodels |>
  predict(new_data = ces2020_test) |>
  cbind(ces2020_test) |>
  conf_mat(truth = voted_for, estimate = .pred_class)
```

When we introduced `tidymodels`, we discussed the importance of randomly constructing training and test sets. We use the training dataset to estimate parameters, and then evaluate the model on the test set. It is natural to ask why we should be subject to the whims of randomness and whether we are making the most of our data. For instance, what if a good model is poorly evaluated because of some random inclusion in the test set? And what if we do not have a large test set?

One commonly used resampling method that goes some way to addressing this is $k$-fold cross-validation. In this approach we create $k$ different samples, or "folds", from the dataset without replacement. We then fit the model to the first $k-1$ folds, and then evaluate it on the last fold. We do this $k$ times, once for every fold, such that every observation will be used for training $k-1$ times and for testing once. The $k$-fold cross-validation estimate is then the average mean squared error [@islr, p. 181].

We can use `vfold_cv()` from `tidymodels` to create, say, 10 folds.

```{r}
set.seed(853)

ces2020_10_folds <- vfold_cv(ces2020, v = 10)
```

We then fit our model across the different combinations of folds with `fit_resamples()`. In this case, the model will be fit 10 times.

```{r}
ces2020_cross_validation <-
  fit_resamples(
    object = logistic_reg(mode = "classification") |> set_engine("glm"),
    preprocessor = recipe(
      voted_for ~ gender + education,
      data = ces2020
    ),
    resamples = ces2020_10_folds,
    metrics = metric_set(accuracy, sens, spec),
    control = control_resamples(save_pred = TRUE)
  )
```

We might be interested to understand the performance of our model and we can use `collect_metrics()` to aggregate them across the folds (@tbl-metricsvoters-1). These types of details would typically be mentioned in passing in the main content of the paper, but included in great detail in an appendix. The average accuracy of our model across the folds is 0.61, while the average sensitivity is 0.19 and the average specificity is 0.90.

```{r}
#| label: tbl-metricsvoters
#| tbl-cap: "Average metrics across the 10 folds of a logistic regression to forecast voter preference"
#| layout-ncol: 2
#| tbl-subcap: ["Key performance metrics", "Confusion matrix"]

collect_metrics(ces2020_cross_validation) |>
  select(.metric, mean) |>
  knitr::kable(
    col.names = c(
      "Metric",
      "Mean"
    ),
    digits = 2,
    booktabs = TRUE,
    linesep = "",
    format.args = list(big.mark = ",")
  )

conf_mat_resampled(ces2020_cross_validation) |>
  mutate(Proportion = Freq / sum(Freq)) |> 
  knitr::kable(
    digits = 2,
    booktabs = TRUE,
    linesep = "",
    format.args = list(big.mark = ",")
  )
```

What does this mean? Accuracy is the proportion of observations that were correctly classified. The result of 0.61 suggests the model is doing better than a coin toss, but not much more. Sensitivity is the proportion of true observations that are identified as true [@islr, p. 145]. In this case would mean that the model predicted a respondent voted for Trump and they did. And specificity is the proportion of false observations that are identified as false [@islr, p. 145]. In this case it is the proportion of voters that voted for Biden, that were predicted to vote for Biden. This confirms our initial thought that model is having trouble identifying Trump supporters.

We can see this in more detail by looking at the confusion matrix (@tbl-metricsvoters-2). When used with resampling approaches, such as cross-validation, the confusion matrix is computed for each fold and then averaged. The model is predicting Biden much more than we might expect from our knowledge of how close the 2020 election was. It suggests that our model may need additional variables to do a better job.

Finally, it may be the case that we are interested in individual-level results, and we can add these to our dataset with `collect_predictions()`.

```{r}
ces2020_with_predictions <-
  cbind(
    ces2020,
    collect_predictions(ces2020_cross_validation) |>
      arrange(.row) |>
      select(.pred_class)
  ) |>
  as_tibble()
```

For instance, we can see that model is essentially predicting support for Biden for all individuals apart from males with no high school, high school graduates, or 2 years of college (@tbl-omgthismodelishorriblelol).

```{r}
#| label: tbl-omgthismodelishorriblelol
#| tbl-cap: "The model is forecasting support for Biden for all females, and for many males, regardless of education"

ces2020_with_predictions |>
  group_by(gender, education, voted_for) |>
  count(.pred_class) |>
  knitr::kable(
    col.names = c(
      "Gender",
      "Education",
      "Voted for",
      "Predicted vote",
      "Number"
    ),
    digits = 0,
    booktabs = TRUE,
    linesep = "",
    format.args = list(big.mark = ",")
  )
```
<!-- this was a really nice practical work through :) -->

## Poisson regression

When we have count data we should initially think to take advantage of the Poisson distribution. The Poisson distribution is governed by one parameter, $\lambda$. This distributes probabilities over non-negative integers and hence governs the shape of the distribution. As such, the Poisson distribution has the interesting feature that the mean is also the variance, and so as the mean increases, so does the variance. The Poisson probability mass function is [@pitman, p. 121]: 
$$P_{\lambda}(k) = e^{-\lambda}\lambda^k/k!\mbox{, for }k=0,1,2,...$$
We can simulate $n=20$ draws from the Poisson distribution with `rpois()`, where $\lambda$ here is equal to three. 

```{r}
rpois(n = 20, lambda = 3)
```

We can also look at what happens to the distribution as we change the value of $\lambda$ (@fig-poissondistributiontakingshape).

```{r}
#| eval: true
#| include: true
#| message: false
#| warning: false
#| fig-cap: "The Poisson distribution is governed by the value of the mean, which is the same as its variance"
#| label: fig-poissondistributiontakingshape

set.seed(853)

poisson_takes_shape <-
  tibble(
    lambda = c(),
    draw = c()
  )

number_of_each <- 100

for (i in c(0, 1, 2, 4, 7, 10, 15, 25, 50)) {
  draws_i <-
    tibble(
      lambda = c(rep(i, number_of_each)),
      draw = c(rpois(n = number_of_each, lambda = i))
    )

  poisson_takes_shape <- rbind(poisson_takes_shape, draws_i)
  rm(draws_i)
}

poisson_takes_shape |>
  mutate(
    lambda = paste("Lambda =", lambda),
    lambda = factor(
      lambda,
      levels = c(
        "Lambda = 0",
        "Lambda = 1",
        "Lambda = 2",
        "Lambda = 4",
        "Lambda = 7",
        "Lambda = 10",
        "Lambda = 15",
        "Lambda = 25",
        "Lambda = 50"
      )
    )
  ) |>
  ggplot(aes(x = draw)) +
  geom_density() + #suggest maybe a bar here instead to keep the PMF 
  facet_wrap(
    vars(lambda),
    scales = "free_y"
  ) +
  theme_minimal() +
  labs(
    x = "Integer",
    y = "Density"
  )
```

To illustrate the situation, we could simulate data about the number of A grades that are awarded in each university course. In this simulated example, we consider three departments, each of which has many courses. Each course will award a different number of A grades.

```{r}
set.seed(853)

class_size <- 26

count_of_A <-
  tibble(
    # From Chris DuBois: https://stackoverflow.com/a/1439843
    department = c(rep.int("1", 26), rep.int("2", 26), rep.int("3", 26)),
    course = c(
      paste0("DEP_1_", letters),
      paste0("DEP_2_", letters),
      paste0("DEP_3_", letters)
    ),
    number_of_A = c(
      rpois(n = class_size, lambda = 5),
      rpois(n = class_size, lambda = 10),
      rpois(n = class_size, lambda = 20)
    )
  )

count_of_A
```

```{r}
#| echo: true
#| eval: true
#| message: false
#| warning: false
#| fig-cap: "Simulated number of A grades in various classes across three departments"
#| label: fig-simgradesdepartments

count_of_A |>
  ggplot(aes(x = number_of_A)) +
  geom_histogram(aes(fill = department), position = "dodge") +
  labs(
    x = "Number of A grades awarded",
    y = "Number of classes",
    fill = "Department"
  ) +
  theme_classic() +
  scale_fill_brewer(palette = "Set1")
```

Our simulated dataset has the number of A grades awarded by courses, which are structured within departments (@fig-simgradesdepartments). In @sec-multilevel-regression-with-post-stratification, we will take advantage of this departmental structure, but for now we just ignore it.

The model that we are interested in estimating is:

$$
\begin{aligned}
y_i|\lambda_i &\sim \mbox{Poisson}(\lambda_i)\\
\log(\lambda_i) & = \beta_0 + \beta_1 x_i
\end{aligned}
$$
where $y_i$ is the number of A grades awarded, and $x_i$ is the course.

We can use `glm()` from base R to get a quick sense of the data. This function is quite general, and we specify Poisson regression by setting the "family" parameter. The estimates are contained in the first column of @tbl-modelsummarypoisson.

```{r}
grades_base <-
  glm(
    number_of_A ~ department,
    data = count_of_A,
    family = "poisson"
  )
```

As with logistic regression, the interpretation of the coefficients from Poisson regression can be difficult. The interpretation of the coefficient on "department2" is that it is the log of the expected difference between departments. We expect $e^{0.883} \approx 2.4$ and $e^{1.703} \approx 5.5$ as many A grades in departments 2 and 3, respectively, compared with department 1 (@tbl-modelsummarypoisson).

If we were interested in prediction, then we could use `tidymodels` to estimate Poisson models with `poissonreg` [@poissonreg] (@tbl-modelsummarypoisson).

```{r}
library(poissonreg)

set.seed(853)

count_of_A_split <-
  initial_split(count_of_A, prop = 0.80)
count_of_A_train <- training(count_of_A_split)
count_of_A_test <- testing(count_of_A_split)

grades_tidymodels <-
  poisson_reg(mode = "regression") |>
  set_engine("glm") |>
  fit(
    number_of_A ~ department,
    data = count_of_A_train
  )
```

The results of this estimation are in the second column of @tbl-modelsummarypoisson. They are similar to the estimates from `glm()`, but the number of observations is less because of the split.

Finally, we could build a Bayesian model and estimate it with `rstanarm` (@tbl-modelsummarypoisson).

$$
\begin{aligned}
y_i|\lambda_i &\sim \mbox{Poisson}(\lambda_i)\\
\log(\lambda_i) & = \beta_0 + \beta_1 x_i\\
\beta_0 & \sim \mbox{Normal}(0, 2.5)\\
\beta_1 & \sim \mbox{Normal}(0, 2.5)
\end{aligned}
$$

```{r}
#| include: true
#| message: false
#| warning: false
#| eval: false

grades_rstanarm <-
  stan_glm(
    number_of_A ~ department,
    data = count_of_A,
    family = poisson(link = "log"),
    prior = normal(location = 0, scale = 2.5, autoscale = TRUE),
    prior_intercept = normal(location = 0, scale = 2.5, autoscale = TRUE),
    seed = 853
  )

saveRDS(
  grades_rstanarm,
  file = "grades_rstanarm.rds"
)
```

```{r}
#| eval: false
#| include: false
#| message: false
#| warning: false

# INTERNAL
saveRDS(
  grades_rstanarm,
  file = "outputs/model/grades_rstanarm.rds"
)
```

```{r}
#| eval: true
#| include: false
#| message: false
#| warning: false

grades_rstanarm <-
  readRDS(file = "outputs/model/grades_rstanarm.rds")
```

The results are contained in the third column of @tbl-modelsummarypoisson.

```{r}
#| label: tbl-modelsummarypoisson
#| tbl-cap: "Forecasting and explanatory models of marathon times based on five-kilometer run times"

modelsummary::modelsummary(
  list(
    "base R" = grades_base,
    "tidymodels" = grades_tidymodels,
    "rstanarm" = grades_rstanarm
  ),
  statistic = "conf.int"
)
```

As with logistic regression, we can use `marginaleffects()` from [@marginaleffects] to help with interpreting these results. It may be useful to consider how we expect the number of A grades to change as we go from one department to another. @tbl-marginaleffectspoisson suggests that in our dataset classes in Department 2 tend to have around five additional A grades, compared with Department 1. And that classes in Department 3, tend to have around 17 more A grades, compared with Department 1. 

```{r}
#| label: tbl-marginaleffectspoisson
#| tbl-cap: "The estimated difference in the number of A grades awarded at each department"

marginaleffects(grades_rstanarm) |> 
  summary() |> 
  select(-type, -term) |> 
  knitr::kable(
    digits = 2,
    col.names = c("Departmental comparison", "Estimate", "2.5%", "97.5%"),
    booktabs = TRUE,
    linesep = ""
  )
```


### Letters used in *Jane Eyre* 

In an earlier age, @edgeworth1885methods made counts of the dactyls in Virgil's *Aeneid* (@HistData makes this dataset available with `HistData::Dactyl`). Inspired by this we could use `gutenbergr` [@gutenbergr] to get the text of *Jane Eyre* by Charlotte Brontë. We could then consider the first 10 lines of each chapter, count the number of words, and count the number of times either "E" or "e" appears. We are interested to see whether the number of e/E's increases as more words are used. If not, it could suggest that the distribution of e/E's is not consistent, which could be of interest to linguists.

Following the workflow advocated in this book, we first sketch our dataset and model. A quick sketch of what the dataset could look like is @fig-letterssketch. And a quick sketch of our model is @fig-lettersmodel.

::: {#fig-letterss layout-ncol=2 layout-valign="bottom" layout="[[50,10,50]]"}

![Planned counts, by line and chapter, in *Jane Eyre*](figures/IMG_2056.png){#fig-letterssketch}

![Planned expected relationship between count of e/E and number of words in the line](figures/IMG_2075.png){#fig-lettersmodel}

Sketches of the expected dataset and analysis force us to consider what we are interested in
:::

We will simulate a dataset the number of e/E letters is distributed following the Poisson distribution (@fig-simenum).

```{r}
#| echo: true
#| eval: true
#| message: false
#| warning: false
#| fig-cap: "Simulated counts of e/E letters"
#| label: fig-simenum

count_of_e_simulation <-
  tibble(
    chapter = c(rep(1, 10), rep(2, 10), rep(3, 10)),
    line = rep(1:10, 3),
    number_words_in_line = runif(min = 0, max = 15, n= 30) |> round(0),
    number_e = rpois(n = 30, lambda = 10)
    )

count_of_e_simulation |> 
  ggplot(aes(y = number_e, x = number_words_in_line)) +
  geom_point() +
  labs(
    x = "Number of words in line",
    y = "Number of e/E letters in the chapter"
  ) +
  theme_classic() +
  scale_fill_brewer(palette = "Set1")
```

We can now gather and prepare our data.

```{r}
#| eval: false
#| echo: true

library(gutenbergr)

gutenberg_id_of_janeeyre <- 1260

jane_eyre <-
  gutenberg_download(
    gutenberg_id = gutenberg_id_of_janeeyre,
    mirror = "https://gutenberg.pglaf.org/"
  )

jane_eyre

write_csv(jane_eyre, "jane_eyre.csv")
```

We will download it and then use our local copy to avoid overly imposing on the Project Gutenberg servers.

```{r}
#| eval: false
#| echo: false

# INTERNAL

write_csv(jane_eyre, "inputs/jane_eyre.csv")
```

```{r}
#| eval: false
#| echo: true

jane_eyre <- read_csv(
  "jane_eyre.csv",
  col_types = cols(
    gutenberg_id = col_integer(),
    text = col_character()
  )
)

jane_eyre
```

```{r}
#| eval: true
#| echo: false

# INTERNAL

jane_eyre <- read_csv(
  "inputs/jane_eyre.csv",
  col_types = cols(
    gutenberg_id = col_integer(),
    text = col_character()
  )
)

jane_eyre
```

We are interested in only those lines that have content, so we remove those empty lines that are just there for spacing. Then we can create counts of the number of times "e" or "E" occurs in that line, for the first 10 lines of each chapter. For instance, we can look at the first few lines and see that there are 5 e/E's in the first line and 8 in the second.

```{r}
jane_eyre_reduced <-
  jane_eyre |>
  filter(!is.na(text)) |> # Remove empty lines
  mutate(chapter = if_else(
    str_detect(text, "CHAPTER") == TRUE,
    text,
    NA_character_
  )) |> # Find start of chapter
  fill(chapter, .direction = "down") |> # Add chapter number to each line
  group_by(chapter) |>
  mutate(chapter_line = row_number()) |> # Add line number of each chapter
  filter(
    !is.na(chapter),
    chapter_line %in% c(2:11)
  ) |> # Start at 2 to get rid of text "CHAPTER I" etc
  select(text, chapter) |>
  mutate(
    chapter = str_remove(chapter, "CHAPTER "),
    chapter = str_remove(chapter, "—CONCLUSION"),
    chapter = as.integer(as.roman(chapter))
  ) # Change chapters to integers

jane_eyre_reduced <-
  jane_eyre_reduced |>
  mutate(count_e = str_count(text, "e|E")) |> 
  # Based on: https://stackoverflow.com/a/38058033
  mutate(word_count = str_count(text, "\\w+"))
```


```{r}
jane_eyre_reduced |> 
  select(chapter, word_count, count_e, text) |> 
  head()
```

We can verify that the mean and variance of the number of E/e's is roughly similar by plotting all of the data (@fig-janeecounts). The mean, in pink, is 6.7, and the variance, in blue, is 6.2. While they are not entirely the same, they are similar. We include the diagonal in @fig-janeecounts-2 to help with thinking about the data. If the data were on the $y=x$ line, then on average the is one E/e per word. As it is below that point, means that on average there is less than one per word.

```{r}
#| echo: true
#| eval: true
#| fig-cap: "Number of 'E' or 'e' in the first ten lines of each chapter in Jane Eyre"
#| label: fig-janeecounts
#| message: false
#| warning: false
#| layout-ncol: 2
#| fig-subcap: ["Distribution of the number of e/E's", "Comparison of the number of e/E's in the line and the number of words in the line"]

mean_e <- mean(jane_eyre_reduced$count_e)
variance_e <- var(jane_eyre_reduced$count_e)

jane_eyre_reduced |>
  ggplot(aes(x = count_e)) +
  geom_histogram() +
  geom_vline(xintercept = mean_e, linetype = "dashed", color = "#C64191") +
  geom_vline(xintercept = variance_e, linetype = "dashed", color = "#0ABAB5") +
  theme_minimal() +
  labs(
    y = "Count",
    x = "Number of e's per line for first ten lines"
  )

jane_eyre_reduced |>
  ggplot(aes(x = word_count, y = count_e)) +
  geom_jitter(alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  theme_minimal() +
  labs(
    x = "Number of words in the line",
    y = "Number of e/E's in the line"
  )
```

We could consider the following model:

$$
\begin{aligned}
y_i|\lambda_i &\sim \mbox{Poisson}(\lambda_i)\\
\log(\lambda_i) & = \beta_0 + \beta_1 x_i\\
\beta_0 & \sim \mbox{Normal}(0, 2.5)\\
\beta_1 & \sim \mbox{Normal}(0, 2.5)
\end{aligned}
$$
where $y_i$ is the number of e/E's in the line and $x_i$ is the number of words in the line. We could estimate the parameters using `stan_glm()`. 

```{r}
#| eval: false
#| echo: true
#| message: false
#| warning: false

jane_e_counts <-
  stan_glm(
    count_e ~ word_count,
    data = jane_eyre_reduced,
    family = poisson(link = "log"),
    seed = 853
  )

saveRDS(
  jane_e_counts,
  file = "jane_e_counts.rds"
)
```

```{r}
#| eval: false
#| echo: false

# INTERNAL

saveRDS(
  jane_e_counts,
  file = "outputs/model/jane_e_counts.rds"
)
```

```{r}
#| eval: true
#| include: false
#| message: false
#| warning: false

library(rstanarm)

jane_e_counts <-
  readRDS(file = "outputs/model/jane_e_counts.rds")
```

<!-- (@tbl-modelsummaryjanee). -->

<!-- ```{r} -->
<!-- #| label: tbl-modelsummaryjanee -->
<!-- #| tbl-cap: "Forecasting and explanatory models of whether it is day or night, based on the number of cars on the road" -->

<!-- modelsummary::modelsummary( -->
<!--   list( -->
<!--     "rstanarm" = jane_e_counts -->
<!--   ), -->
<!--   statistic = "conf.int" -->
<!-- ) -->
<!-- ``` -->

While we would normally be interested in the table of estimates, as we have seen that a few times now, rather than again creating a table of the estimates, we introduce `plot_cap()` from `marginaleffects` [@marginaleffects]. We can use this to show the number of e/E's predicted by the model, for each line, based on the number of words in that line. @fig-predictionsjaneecounts makes it clear that we expect a positive relationship.

```{r}
#| label: fig-predictionsjaneecounts
#| fig-cap: "The predicted number of e/E's in each line based on the number of words"

plot_cap(jane_e_counts, condition = "word_count") +
  labs(x = "Number of words",
       y = "Average number of e/E in the first 10 lines")
```

## Negative binomial regression

One of the major restrictions with Poisson regression is the assumption that the mean and the variance are the same. We can relax this assumption to allow over-dispersion and can use a close variant, negative binomial regression. 

<!-- Before getting to this, it might be worth taking a moment to think about why we are we going through all this trouble. Common practice is to change from counts into a binary and then use logistic regression. The issue with this is that we may lose information when we group responses together. -->

Poisson and negative binomial models go hand in hand. It is often the case that we will end up fitting both, and then comparing them. For instance, @maher1982modelling considers both in the context of results from the English Football League and discusses situations in which one may be considered more appropriate than the other. Similarly, @thanksleo considers the 2000 US presidential election and especially the issue of overdispersion in a Poisson analysis. And @Osgood2000 compares them in the case of crime data.

<!-- The shape of the negative binomial distribution is determined by two parameters, the probability of success, $p$, and the number of successes, $r$ [@pitman, p. 482]:  -->

<!-- $$P(F_r = n) = {n+r-1\choose r-1}p^r(1-p)^n\mbox{, for }n=0,1,2,...$$ -->
<!-- where $F_r$ is the number of failures, before the $r$th success in Bernoulli trials. -->

<!-- For instance, if we were to consider the number of k's in those same lines of *Jane Eyre*, then we would have many zeros (@fig-janeecountsk).  -->

<!-- ```{r} -->
<!-- jane_eyre_reduced <- -->
<!--   jane_eyre_reduced |> -->
<!--   mutate(count_k = str_count(text, "k|K")) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- #| echo: true -->
<!-- #| eval: true -->
<!-- #| fig-cap: "Number of 'K' or 'k' in the first ten lines of each chapter in Jane Eyre" -->
<!-- #| label: fig-janeecountsk -->
<!-- #| message: false -->
<!-- #| warning: false -->

<!-- mean_k <- mean(jane_eyre_reduced$count_k) -->
<!-- variance_k <- var(jane_eyre_reduced$count_k) -->

<!-- jane_eyre_reduced |> -->
<!--   ggplot(aes(y = chapter, x = count_k)) + -->
<!--   geom_point(alpha = 0.5) + -->
<!--   geom_vline(xintercept = mean_k, linetype = "dashed", color = "#C64191") + -->
<!--   geom_vline(xintercept = variance_k, linetype = "dashed", color = "#0ABAB5") + -->
<!--   theme_minimal() + -->
<!--   labs( -->
<!--     x = "Number of k's per line for first ten lines", -->
<!--     y = "Chapter" -->
<!--   ) -->
<!-- ``` -->

### Mortality in Alberta

Consider, somewhat morbidly, that every year each individual either dies or does not. From the perspective of a geographic area, we could gather data on the number of people who died each year, by their cause of death. The Canadian province of Alberta makes [available](https://open.alberta.ca/opendata/leading-causes-of-death) the number of deaths, by cause, since 2001, for the top 30 causes each year. 

As always we first sketch our dataset and model. A quick sketch of what the dataset could look like is @fig-albertadatasketch And a quick sketch of our model is @fig-albertamodelsketch

::: {#fig-letterss layout-ncol=2 layout-valign="bottom"}

![Quick sketch of a dataset that could be used to examine cause of death in Alberta](figures/IMG_2076.png){#fig-albertadatasketch}

![Quick sketch of what we expect from the analysis of cause of death in Alberta before finalizing either the data or the analysis](figures/IMG_2061.png){#fig-albertamodelsketch}

Sketches of the expected dataset and analysis for cause of death in Alberta
:::

We will simulate a dataset of cause of death distributed following the negative binomial distribution.

```{r}
alberta_death_simulation <-
  tibble(
    cause = rep(x = c("Heart", "Stroke", "Diabetes"), times = 10),
    year = rep(x = 2016:2018, times = 10),
    deaths = rnbinom(n = 30, size = 20, prob = 0.1)
    )

alberta_death_simulation
```

We download that data, and save it as "alberta_COD.csv". We can look at the distribution of these deaths, by year and cause (@fig-albertacod). We have truncated the full cause of death because some are quite long. As some causes are not always in the top 30 each year, not all causes have the same number of occurrences.

<!-- :::{.callout-note} -->
<!-- ## Oh, you think we have good data on that! -->

<!-- Cause of death -->
<!-- ::: -->

```{r}
#| eval: false
#| echo: true

alberta_cod <-
  read_csv("alberta_COD.csv",
           col_types = cols(
             `Calendar Year` = col_integer(),
             Cause = col_character(),
             Ranking = col_integer(),
             `Total Deaths` = col_integer()
             )) |>
  janitor::clean_names() |>
  add_count(cause) |>
  mutate(cause = str_trunc(cause, 30))
```


```{r}
#| eval: true
#| echo: false

alberta_cod <-
  read_csv("inputs/alberta_COD.csv",
           col_types = cols(
             `Calendar Year` = col_integer(),
             Cause = col_character(),
             Ranking = col_integer(),
             `Total Deaths` = col_integer()
             )
           ) |>
  janitor::clean_names() |>
  add_count(cause) |>
  mutate(cause = str_trunc(cause, 30))
```

If we were to look at the top ten causes in 2021, we would notice a variety of interesting aspects (@tbl-albertahuh). For instance, we would expect that the most common causes would be in all 21 years of our data. But we notice that that most common cause, "Other ill-defined and unknown causes of mortality", is only in three years. Naturally, "COVID-19, virus identified", is only in two other years, as there were no COVID deaths in Canada before 2020.

```{r}
#| label: tbl-albertahuh
#| tbl-cap: "Top ten causes of death in Alberta in 2021"
#| warning: false

alberta_cod |>
  filter(calendar_year == 2021,
         ranking <= 10) |>
  mutate(total_deaths = format(total_deaths, big.mark = ",")) |>
  knitr::kable(
    col.names = c("Year",
                  "Cause",
                  "Ranking",
                  "Total deaths",
                  "Number of years"),
    digits = 0,
    align = c("l", "r", "r", "r", "r"),
    booktabs = TRUE,
    linesep = ""
  )
```

In this case, for simplicity, we will restrict ourselves to the five most common causes of death in 2021, that have been present every year.

```{r}
alberta_cod_top_five <- 
  alberta_cod |> 
  filter(calendar_year == 2021,
         n == 21) |> 
  slice_max(order_by = desc(ranking), n = 5) |> 
  pull(cause)

alberta_cod <- 
  alberta_cod |> 
  filter(cause %in% alberta_cod_top_five)
```


```{r}
#| fig-cap: "Annual number of deaths for the top 5 causes in 2021, since 2001, for Alberta, Canada"
#| label: fig-albertacod
#| message: false
#| warning: false

alberta_cod |>
  ggplot(aes(x = calendar_year, y = total_deaths, color = cause)) +
  geom_line(alpha = 0.5) +
  theme_minimal() +
  scale_color_brewer(palette = "Set1") +
  labs(
    x = "Year",
    y = "Annual number of deaths in Alberta",
    color = "Cause"
  )
```

One thing that we notice is that the mean, 1,273, is different to the variance, 182,378 (@tbl-ohboyalberta).

```{r}
#| echo: false
#| eval: true
#| label: tbl-ohboyalberta
#| tbl-cap: "Summary statistics of the number of yearly deaths, by cause, in Alberta"
library(modelsummary)

datasummary(total_deaths ~ Min + Mean + Max + SD + Var + N,
            fmt = 0,
            data = alberta_cod)
```

We can implement negative binomial regression when using `stan_glm()` by specifying that distribution in "family". In this case, we run both Poisson and negative binomial.

```{r}
#| echo: true
#| eval: false
#| message: false
#| warning: false

cause_of_death_alberta_poisson <-
  stan_glm(
    total_deaths ~ cause,
    data = alberta_cod,
    family = poisson(link = "log"),
    seed = 853
  )

cause_of_death_alberta_neg_binomial <-
  stan_glm(
    total_deaths ~ cause,
    data = alberta_cod,
    family = neg_binomial_2(link = "log"),
    seed = 853
  )
```


```{r}
#| echo: false
#| eval: false

# INTERNAL

saveRDS(
  cause_of_death_alberta_poisson,
  file = "outputs/model/cause_of_death_alberta_poisson.rds"
)

saveRDS(
  cause_of_death_alberta_neg_binomial,
  file = "outputs/model/cause_of_death_alberta_neg_binomial.rds"
)
```

```{r}
#| eval: true
#| echo: false

cause_of_death_alberta_poisson <-
  readRDS(file = "outputs/model/cause_of_death_alberta_poisson.rds")

cause_of_death_alberta_neg_binomial <-
  readRDS(file = "outputs/model/cause_of_death_alberta_neg_binomial.rds")
```

We can compare our different models (@tbl-modelsummarypoissonvsnegbinomial) (we need to load `broom.mixed` [@mixedbroom] because this is needed under the hood).

```{r}
#| label: tbl-modelsummarypoissonvsnegbinomial
#| tbl-cap: "Modelling the most prevelent cause of deaths in Alberta, 2001-2020"

library(broom.mixed)

modelsummary::modelsummary(
  list(
    "Poisson" = cause_of_death_alberta_poisson,
    "Negative binomial" = cause_of_death_alberta_neg_binomial
  ),
  statistic = "conf.int"
)
```

We could use posterior predictive checks to more clearly show that the negative binomial approach is a better choice for this circumstance (@fig-ppcheckpoissonvsbinomial). <--! jackmans pscl package has an easy implementation of the over-dispersion test, perhaps worth mentioning briefly! -->

```{r}
#| echo: true
#| eval: true
#| message: false
#| warning: false
#| label: fig-ppcheckpoissonvsbinomial
#| layout-ncol: 2
#| fig-cap: "Comparing posterior prediction checks for Poisson and negative binomial models"
#| fig-subcap: ["Poisson model", "Negative binomial model"]

pp_check(cause_of_death_alberta_poisson) +
  theme(legend.position = "bottom")

pp_check(cause_of_death_alberta_neg_binomial) +
  theme(legend.position = "bottom")
```

Finally, we can compare between the models using the resampling method leave-one-out (LOO) cross-validation. This is a variant of cross-validation, introduced earlier, where the size of each fold is one. That is to say, if there was a dataset with 100 observations this LOO is equivalent to 100-fold cross validation. We can implement this in `rstanarm` with `loo()` for each model, and then compare between them with `loo_compare()`. Strictly speaking LOO-CV is not actually done by `loo()`, because it would be too computationally intensive. Instead an approximation is done provides the expected log point wise predictive density (ELPD), where the higher the better.

```{r}
#| message: false
#| warning: false

poisson <- loo(cause_of_death_alberta_poisson, cores = 2)
neg_binomial <- loo(cause_of_death_alberta_neg_binomial, cores = 2)

loo_compare(poisson, neg_binomial)
```

In this case we find that the negative binomial model is a better fit than the Poisson, because ELPD is larger.

We have covered a variety of diagnostic aspects for Bayesian models. While it is difficult to be definitive about what is "enough" because it is context specific, the following checklist would be enough for most purposes. These would go into an appendix that was mentioned and cross-referenced in the model section of a paper:

- Prior predictive checks.
- Trace plots.
- Rhat plots.
- Posterior distributions.
- Posterior predictive checks.



<!-- ### Proportional hazards -->

<!-- Could do like how long someone has to wait for something? -->




## Deploying models

Having done the work to develop a dataset and explore it with a model that we are confident can be used, we may wish to enable this to be used more widely than just our own computer. There are a variety of ways of doing this, including:

- using the cloud,
- creating R packages,
- making `shiny` applications, and
- using `plumber` to create an API.

The general idea here is that we need to know, and allow others to come to trust, the whole workflow. That is what our approach to this point brings. After this, then we may like to use our model more broadly. Say we have been able to scrape some data from a website, bring some order to that chaos, make some charts, appropriately model it, and write this all up. In most academic settings that is more than enough. But in many industry settings we would like to use the model to do something. For instance, setting up a website that allows a model to be used to generate an insurance quote given several inputs.

In this chapter, we begin by moving our compute from our local computer to the cloud. We then describe the use of R packages and Shiny for sharing models. That works well, but in some settings other users may like to interact with our model in ways that we are not focused on. One way to allow this is to make our results available to other computers, and for that we will want to make an APIs. Hence, we introduce `plumber` [@plumber], which is a way of creating APIs.

### Amazon Web Services

Apocryphally the cloud is just another name for someone else's computer. And while that is true to various degrees, for our purposes that is enough. Learning to use someone else's computer can be great for a number of reasons including:

- Scalability: It can be quite expensive to buy a new computer, especially if we only need it to run something every now and then, but by using the cloud, we can just rent for a few hours or days. This allows use to amortize this cost and work out what we actually need before committing to a purchase. It also allows us to easily increase or decrease the compute scale if we suddenly have a substantial increase in demand.
- Portability: If we can shift our analysis workflow from a local computer to the cloud, then that suggests that there is we are likely doing good things in terms of reproducibility and portability. At the very least, code can run both locally and on the cloud, which is a big step in terms of reproducibility.
- Set-and-forget: If we are doing something that will take a while, then it can be great to not have to worry about our own computer needing to run overnight. Additionally, on many cloud options, open-source statistical software, such as R and Python, is either already available, or relatively easy to set-up.

That said, there are downsides, including:

- Cost: While most cloud options are cheap, they are rarely free. To provide an idea of cost, using a well-featured AWS instance for a few days may end up being a few dollars. It is also easy to accidentally forget about something, and generate unexpectedly large bills, especially initially.
- Public: It can be easy to make mistakes and accidentally make everything public.
- Time: It takes time to get set-up and comfortable on the cloud.

When we use the cloud, we are typically running code on a "virtual machine" (VM). This is an allocation that is part of a larger collection of computers that has been designed to act like a computer with specific features. For instance, we may specify that our virtual machine has, say, 8 GB RAM, 128 storage, and 4 CPUs. The VM would then act like a computer with those specifications. The cost to use cloud options increases based on the VM specifications.

In a sense, we started with a cloud option, through our initial recommendation in @sec-fire-hose of using RStudio Cloud, before we moved to our local computer in @sec-r-essentials. That cloud option was specifically designed for beginners. We will now introduce a more general cloud option: Amazon Web Services (AWS). Often a particular business will use a particular cloud option, such as Google, AWS, or Azure, but developing familiarity with one will make the use of the others easier.

Amazon Web Services is a cloud service from Amazon. To get started we need to create an AWS Developer account [here](https://aws.amazon.com/developer/) (@fig-awsone).

::: {#fig-ipums layout-nrow=2}

![AWS Developer website](figures/aws_one.png){#fig-awsone}

![AWS Developer console](figures/aws_two.png){#fig-awstwo}

![Launching an AWSinstance](figures/aws_three.png){#fig-awsthree}

![Establishing a key-pair](figures/aws_five.png){#fig-awsfive}

Overview of getting Amazon AWS set-up
:::

After we have created an account, we need to select a region where the computer that we will access is located. After this, we want to "Launch a virtual machine" with EC2 (@fig-awstwo).

The first step is to choose an Amazon Machine Image (AMI). This provides the details of the computer that you will be using. For instance, a local computer may be a MacBook running Monterey. Louis Aslett provides AMIs that are already set-up with RStudio and much else [here](http://www.louisaslett.com/RStudio_AMI/). We can either search for the AMI of the region that we registered for, or click on the relevant link on Aslett's website. For instance, to use the AMI set-up for the Canadian central region we search for "ami-0bdd24fd36f07b638". The benefit of using these AMIs is that they are set-up specifically for RStudio, but the trade-off is that they are a little outdated, as they were compiled in August 2020.

In the next step we can choose how powerful the computer will be. The free tier is basic computer, but we can choose better ones when we need them. At this point we can pretty much just launch the instance (@fig-awsthree). If we start using AWS more seriously, then we could go back and select different options, especially around the security of the account. AWS relies on key pairs. And so, we will need to create a PEM and save it locally (@fig-awsfive). We can then launch the instance.

After a few minutes, the instance will be running. We can use it by pasting the "public DNS" into a browser. The username is "rstudio" and the password is the instance ID.

We should have RStudio running, which is exciting. The first thing to do is probably to change the default password using the instructions in the instance.

We do not need to install, say, the `tidyverse`, instead we can just call the library and keep going. This is because this AMI comes with many packages already installed. We can see the list of packages that are installed with `installed.packages()`. For instance, `rstan` is already installed, and we could set-up an instance with GPUs if we needed.

Perhaps as important as being able to start an AWS instance is being able to stop it (so that we do not get billed). The free tier is useful, but we do need to turn it off. To stop an instance, in the AWS instances page, select it, then "Actions -> Instance State -> Terminate".






<!-- ## R packages -->

<!-- To this point we have largely been using R Packages to do things for us. However, another way is to have them loaded -->



<!-- ## Shiny -->



### Plumber and model APIs

The general idea behind the `plumber` package [@plumber] is that we can train a model and make it available via an API that we can call when we want a forecast.

Just to get something working, let us make a function that returns "Hello Toronto" regardless of the output. Open a new R file, add the following, and then save it as "plumber.R" (you may need to install the `plumber` package if you have not done that yet).

```{r}
#| eval: false

library(plumber)

#* @get /print_toronto
print_toronto <- function() {
  result <- "Hello Toronto"
  return(result)
}
```

After that is saved, in the top right of the editor you should get a button to "Run API". Click that, and your API should load. It will be a "Swagger" application, which provides a GUI around our API. Expand the GET method, and then click "Try it out" and "Execute". In the response body, you should get "Hello Toronto". 

To more closely reflect the fact that this is an API designed for computers, you can copy/paste the "Request URL" into a browser and it should return "Hello Toronto".


#### Local model

Now, we are going to update the API so that it serves a model output, given some input. This follows @buhrplumber.

At this point, we should start a new R Project. To get started, let us simulate some data and then train a model on it. In this case we are interested in forecasting how long a baby may sleep overnight, given we know how long they slept during their afternoon nap.

```{r} 
#| warning: false
#| message: false
#| eval: true

library(tidyverse)
set.seed(853)

number_of_observations <- 1000

baby_sleep <-
  tibble(
    afternoon_nap_length = rnorm(number_of_observations, 120, 5) |> abs(),
    noise = rnorm(number_of_observations, 0, 120),
    night_sleep_length = afternoon_nap_length * 4 + noise,
  )

baby_sleep |>
  ggplot(aes(x = afternoon_nap_length, y = night_sleep_length)) +
  geom_point(alpha = 0.5) +
  labs(
    x = "Baby's afternoon nap length (minutes)",
    y = "Baby's overnight sleep length (minutes)"
  ) +
  theme_classic()
```

Let us now use `tidymodels` to quickly make a model.

```{r} 
#| warning: false
#| message: false
#| eval: false

library(tidymodels)

set.seed(853)

baby_sleep_split <- initial_split(baby_sleep, prop = 0.80)
baby_sleep_train <- training(baby_sleep_split)
baby_sleep_test <- testing(baby_sleep_split)

model <-
  linear_reg() |>
  set_engine(engine = "lm") |>
  fit(
    night_sleep_length ~ afternoon_nap_length,
    data = baby_sleep_train
  )

write_rds(x = model, file = "baby_sleep.rds")
```

At this point, we have a model. One difference from what you might be used to is that we have saved the model as an ".rds" file. We are going to read that in.

Now that we have our model we want to put that into a file that we will use the API to access, again called "plumber.R". And we also want a file that sets up the API, called "server.R". Make an R script called "server.R" and add the following content:

```{r}
#| eval: false

library(plumber)

serve_model <- plumb("plumber.R")
serve_model$run(port = 8000)
```

Then in "plumber.R" add the following content:

```{r}
#| eval: false

library(plumber)
library(tidyverse)

model <- readRDS("baby_sleep.rds")

version_number <- "0.0.1"

variables <-
  list(
    afternoon_nap_length = "A value in minutes, likely between 0 and 240.",
    night_sleep_length = "A forecast, in minutes, likely between 0 and 1000."
  )

#* @param afternoon_nap_length
#* @get /survival
predict_sleep <- function(afternoon_nap_length = 0) {
  afternoon_nap_length <- as.integer(afternoon_nap_length)

  payload <- data.frame(afternoon_nap_length = afternoon_nap_length)

  prediction <- predict(model, payload)

  result <- list(
    input = list(payload),
    response = list("estimated_night_sleep" = prediction),
    status = 200,
    model_version = version_number
  )

  return(result)
}
```

Again, after we save the "plumber.R" file we should have an option to "Run API". Click that and you can try out the API locally in the same way as before. In this case, click "Try It Out" and then input an afternoon nap length in minutes. The response body will contain the prediction based on the data and model we set up.


#### Cloud model

<!-- okay just one quick suggestion on the cloud stuff. I wonder if good way to motivate against the *why bother* question would be a brief discussion of the relationship between the number of observations/parameters and computing time. illustrate quadratic growth in complexity vs linear. Should help people understand which kind of problems you'd need a server for... -->

To this point, we have got an API working on our own machine, but what we really want to do is to get it working on a computer such that the API can be accessed by anyone. To do this we are going to use [DigitalOcean](https://www.digitalocean.com). It is a charged service, but when you create an account, it will come with $200 in credit, which will be enough to get started.

This set-up process will take some time, but we only need to do it once. Two additional packages that will assist here are `plumberDeploy` [@plumberdeploy] and `analogsea` [@citeanalogsea] (which will need to be installed from GitHub: `remotes::install_github("sckott/analogsea")`).

Now we need to connect the local computer with the DigitalOcean account. 

```{r}
#| eval: false

library(analogsea)

account()
```

Now we need to authenticate the connection, and this is done using a SSH public key.

```{r}
#| eval: false

key_create()
```

What you want is to have a ".pub" file on our computer. Then copy the public key aspect in that file, and add it to the SSH keys section in the account security settings. When we have the key on our local computer, then we can check this using `ssh`.

```{r}
#| eval: false
library(ssh)

ssh_key_info()
```

Again, this will all take a while to validate. DigitalOcean calls every computer that we start a "droplet". If we start three computers, then we will have started three droplets. We can check the droplets that are running.
 
```{r}
#| eval: false

droplets()
```

If everything is set-up properly, then this will print the information about all droplets that you have associated with the account (which at this point, is probably none). We must first create a droplet.

```{r}
#| eval: false
library(plumberDeploy)

id <- do_provision(example = FALSE)
```

Then we get asked for the SSH passphrase and then it will just set-up a bunch of things. After this we are going to need to install a whole bunch of things onto our droplet.

```{r}
#| eval: false

install_r_package(
  droplet = id,
  c(
    "plumber",
    "remotes",
    "here"
  )
)

debian_apt_get_install(
  id,
  "libssl-dev",
  "libsodium-dev",
  "libcurl4-openssl-dev"
)

debian_apt_get_install(
  id,
  "libxml2-dev"
)

install_r_package(
  id,
  c(
    "config",
    "httr",
    "urltools",
    "plumber"
  )
)

install_r_package(id, c("xml2"))
install_r_package(id, c("tidyverse"))
install_r_package(id, c("tidymodels"))
```

And then when that is finally set-up (it will take 30 minutes or so) we can deploy our API.

```{r}
#| eval: false

do_deploy_api(
  droplet = id,
  path = "example",
  localPath = getwd(),
  port = 8000,
  docs = TRUE,
  overwrite = TRUE
)
```








## Exercises and tutorial


### Exercises {.unnumbered}

1. *(Plan)* Consider the following scenario: *A person is interested in the heights of all the buildings in London. They walk around the city counting the number of floors for each building.* Please sketch out what that dataset could look like and then sketch a graph that you could build to show all observations.
2. *(Simulate)* Please further consider the scenario described and simulate the situation, along with three independent variables that are associated the count of the number of levels.
3. *(Acquire)* Please describe one possible source of such a dataset.
4. *(Explore)* Please use `ggplot2` to build the graph that you sketched.
5. *(Communicate)* Please write two paragraphs about what you did.
6. Please simulate the situation in which there are two independent variables, "race" and "gender", and one dependent variable, "vote_preference", which is imperfectly related to them.
7. Please write a linear relationship between some response variable, Y, and some predictor, X. What is the intercept term? What is the slope term? What would adding a hat to these indicate?
8. What is the least squares criterion? Similarly, what is RSS and what are we trying to do when we run least squares regression?
9. What is statistical bias?
10. If there were three variables: Snow, Temperature, and Wind, please write R code that would fit a simple linear regression to explain Snow as a function of Temperature and Wind. What do you think would be the effect of adding another explanatory variable - daily stock market returns - to your model?
11. According to @greenland2016statistical, p-values test (pick one):
    a. All the assumptions about how the data were generated (the entire model), not just the targeted hypothesis it is supposed to test (such as a null hypothesis).
    b. Whether the hypothesis targeted for testing is true or not.
    c. A dichotomy whereby results can be declared "statistically significant".
12. According to @greenland2016statistical, a p-value may be small because (select all):
    a. The targeted hypothesis is false.
    b. The study protocols were violated.
    c. It was selected for presentation based on its small size. 
13. According to @obermeyer2019dissecting, why does racial bias occur in an algorithm used to guide health decisions in the US (pick one)?
    a. The algorithm uses health costs as a proxy for health needs.
    b. The algorithm was trained on Reddit data.
14. When should we use logistic regression (pick one)?
    a. Continuous dependent variable.
    b. Binary dependent variable.
    c. Count dependent variable.
15. We are interested in studying how voting intentions in the recent US presidential election vary by an individual's income. We set up a logistic regression model to study this relationship. In this study, one possible dependent variable would be (pick one)?
    a. Whether the respondent is a US citizen (yes/no)
    b. The respondent's personal income (high/low)
    c. Whether the respondent is going to vote for Trump (yes/no)
    d. Who the respondent voted for in 2016 (Trump/Clinton)
16. We are interested in studying how voting intentions in the recent US presidential election vary by an individual's income. We set up a logistic regression model to study this relationship. In this study, one possible dependent variable would be (pick one)?
    a. The race of the respondent (white/not white)
    b. The respondent's marital status (married/not)
    c. Whether the respondent is registered to vote (yes/no)
    d. Whether the respondent is going to vote for Biden (yes/no)
17. Please explain what a p-value is, using only the term itself (i.e. "p-value") and words that are amongst the 1,000 most common in the English language according to the [XKCD Simple Writer](https://xkcd.com/simplewriter/). (Please write one or two paragraphs.)
18. The mean of a Poisson distribution is equal to its?
    a. Median.
    b. Standard deviation.
    c. Variance.
19. What is power (in a statistical context)?
20. According to @citemcelreath [p. 162] "Regression will not sort it out. Regression is indeed an oracle, but a cruel one. It speaks in riddles and delights in punishing us for..." (please select one answer)? 
    a. overcomplicating models.
    b.  asking bad questions.
    c. using bad data.
21. Is a model that fits the small or large world more important to you, and why?
22. Please redo the `tidymodels` example of US elections but include additional variables. Which variable did you choose, and how did the performance of the model improve?
23. Please create the graph of the density of the Poisson distribution when $\lambda = 75$.
24. From @gelmanhillvehtari2020, what is the offset in Poisson regression?
25. Redo the *Jane Eyre* example, but for "A/a".


### Tutorial {.unnumbered}

**Option 1:**

Allow that the true data generating process is the Normal distribution with mean of one, and standard deviation of 1. We obtain a sample of 1,000 observations using some instrument. Simulate the following situation:

1) Unknown to us, the instrument has a mistake in it, which means that it has a maximum memory of 900 observations, and begins over-writing at that point, so the final 100 observations are actually a repeat of the first 100. 
2) We employ a research assistant to clean and prepare the dataset. During the process of doing this, unknown to us, they accidentally change half of the negative draws to be positive. 
3) They additionally, accidentally, change the decimal place on any value between 1 and 1.1, so that, for instance 1 becomes 0.1, and 1.1 would become 0.11.
4) You finally get the cleaned dataset and are interested in understanding whether the mean of the true data generating process is greater than 0.

Discuss your analysis of the data, and the effect the issues have had, and finally some steps you can put in place to ensure actual analysis has a chance to flag some of these issues.



**Option 2:**

Please consider @maher1982modelling or @thanksleo. Build a simplified version of their model. Obtain some recent relevant data, estimate the model, and discuss your choice between Poisson and negative binomial regression.



